---
title: 'IST 707 Data Analytic: Homework 04'
author: "Wanyue Xiao"
date: "April 26th, 2020"
output:
  html_document:
    theme: cerulean
  pdf_document: default
---
# Brief Introduction
This homework will analyze the Disease Prediction dataset which records disease related observations. The **purpose** of this homework is to **find essential features and approproate modles to predict that if the patient is getting that disease or not**. To get this estimated result, we should clean the dataset, find important features, and construct required models through using a series of Classification Models, such as **ANN, Logistic Regression, KNN, XGBoost, SVM, Random Forest, and Naive Bayes**. Then, we need to use those models to fit the test dataset and get estimated results. Finally, we will use several model performance evaluation techniques to assess each model and select the most appropriate model for this dataset.
```{r}
```
# Data Importing and Munging
Import dataset, convert data type, remove features with no or low variance, remove duplicates, checking and replacing null values, finding and replacing outliers.
<br>
Before analyzing the dataset, one should library all the necessary packages.
```{r message=FALSE, warning=FALSE}
library(caret) 
library(car)
library(corrplot)
library(dplyr)
library(e1071)
library(factoextra)
library(ggplot2)
library(gplots)
library(ggpubr)
library(glmnet)
library(knitr)
library(keras)
library(klaR)
library(lime)
library(Matrix)
library(naivebayes)
library(naniar)
library(pROC)
library(plyr)
library(rattle)
library(rpart)
library(recipes)
library(RANN)
library(RColorBrewer)
library(tidyr)
library(tidyquant)
library(tidyverse)
library(xgboost)
library(yardstick)
```
Importing the Dataset. 
```{r results='hide'}
my.dir <- getwd()
train <- read.csv(paste0(my.dir, "/","Disease Prediction Training.csv"), header = TRUE, stringsAsFactors = FALSE)
test <- read.csv(paste0(my.dir, "/","Disease Prediction Testing.csv"), header = TRUE, stringsAsFactors = FALSE)
```
**Checking data structure**
<br>
The weather dataset consists of 12 columns. 9 out of these 12 columns contain numeric data while the rest of the columns contain categorical data. Since the test dataset are similar with train dataset, it is unnecessary to display again. 
```{r}
dim(train)
str(train)
summary(train)
```

```{r}
```
## Missing Values Management
Checking the columns with missing values and empty values. We can find that none of these columns has missing value. Here, **neither the train or test dataset has missing value**.
```{r fig.width=10, fig.height= 3}
vis_miss(train)
vis_miss(test)
```
<br>
**Empty Values Management**
<br>
It is possible that those categorcial type columns contain empty value. To find out that, we can elaborate all the factors conatined inside.
```{r}
categorical_columns <- c('Gender', 'Cholesterol', 'Glucose')
for (i in categorical_columns){
  print(unique(train[i]))
}

categorical_columns <- c('Gender', 'Cholesterol', 'Glucose')
for (i in categorical_columns){
  print(unique(test[i]))
}
```

```{r}
```
## Dealing with Duplicates
Checking any possible duplicate data for both weather and test dataset. There are 3061 duplicates inside the dataset. We can go through the top 10 duplicates. Then, all those duplicates should be eliminated.
```{r}
duplicated <- train[duplicated(train),] %>% arrange(Age, Gender, Height, Weight)
dim(duplicated)
head(duplicated,10)
train <- unique(train)
```
We have to manage the test dataset in the same way.
```{r}
test <- unique(test)
```

```{r}
```
## Data Type Conversion
change the original data types.
```{r}
num_var <- sapply(train, is.integer)
train[, num_var] <- lapply(train[, num_var], as.numeric)
char_var <- sapply(train, is.character)
train[, char_var] <- lapply(train[, char_var], as.factor)
```
Managing the test dataset in the same way.
```{r}
num_var <- sapply(test, is.integer)
test[, num_var] <- lapply(test[, num_var], as.numeric)
char_var <- sapply(test, is.character)
test[, char_var] <- lapply(test[, char_var], as.factor)
```

```{r}
```
## 0utliers Management
To find potential outliers, one can use boxplots to visualize the data. For this graph, one can conclude the columns with outliers:
<br>
* Age <br> 
* Heigth <br> 
* Weight <br>
* High.Blood.Pressure <br>
* Low.Blood.Pressure 
<br>
<br>
1. Even though there are several columns with outliers, we have to manage those outliers separately. **Some of them will not be managed since those outliers are plausible to exist in real world**. For example, the minimum of "Age" (which is 29) has been recognized as outlier. However, this number is highly possible to exist. 
<br>
2. **Some of them will be eliminated, replaced, or truncated since it contain very strange value**. Take the blood pressure as an example. It is a common sense that blood pressure could not be negative or higher than 300 mm Hg. After googling, the American Heart Association publiced that there are two numbers in a blood pressure reading, which are the upper (systolic) and lower (diastolic) numbers. Normally, **lower blood pressure ranges between 60 and 140** while **upper blood pressure ranges between 90 and 250**. Besides, it is also **abnormal to find that the value of Lower Blood Pressure is higher than that of High Blood Pressure**. 
<br>
3. What's more, **abnormalies also could be found in "Heigth" and "Weigth" columns**. For example, a male whose heigth is 178 only weighted 11kg.
<br>
```{r warning=FALSE, fig.width=12, fig.height=12, cache=TRUE}
train %>% keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(y = value, fill = "orange")) +
  facet_wrap(~key, scales = "free") +
  geom_boxplot() +
  labs(title = "Boxplots of Numeric Columns") +
  theme_minimal()
```
<br>
<br>
To manage those unrealistic outliers, we can check their quantile values. The 2.5% and 97.5% quantitles for the Low.Blood.Pressure are 60 and 100 while those of values for High.Blood.Pressure are 100 and 170. 
```{r}
quantile(train$Low.Blood.Pressure, c(0.025,0.975))
quantile(train$High.Blood.Pressure, c(0.025,0.975))
```
<br>
Combing the quantile results above and information we obtained from the Internet, we decide to set 60 and 140 as the boundary of Low.Blood.Pressure while to set 90 and 250 as the boundary for High.Blood.Pressure could be 90 and 250.
```{r}
train <- train %>% filter(Low.Blood.Pressure >= 60 & Low.Blood.Pressure <= 140)
train <- train %>% filter(High.Blood.Pressure >= 90 & High.Blood.Pressure <= 250)
train <- train %>% filter(Low.Blood.Pressure < High.Blood.Pressure)
```
<br> 
For outliers in "Heigth" and "Weigth", we can follow the same processes mentioned above. Then, we can decide that the boundary for heigth is 51 and 108 while the boundary for Height is 150 and 180. 
```{r}
quantile(train$Weight, c(0.025,0.975))
quantile(train$Height, c(0.025,0.975))
train <- train %>% filter(Weight >= 51 & Weight <= 108)
train <- train %>% filter(Height >= 150 & Height <= 180)
```
<br>
To better discover the relationship between "Heigth" and "Weigth", we can introduce a new column called "BMI" to the dataset.
```{r}
train$BMI <- as.numeric(train$Weight/((train$Height/100)^2))
```
<br>
Similarly, we have to manage the test dataset in the same way.
```{r}
test <- test %>% filter(Low.Blood.Pressure >= 60 & Low.Blood.Pressure <= 140)
test <- test %>% filter(High.Blood.Pressure >= 90 & High.Blood.Pressure <= 250)
test <- test %>% filter(Low.Blood.Pressure < High.Blood.Pressure)
test <- test %>% filter(Height >= 150 & Height <= 180)
test <- test %>% filter(Weight >= 51 & Weight <= 107)
test$BMI <- as.numeric(test$Weight/((test$Height/100)^2))
```

```{r}

```
## Checking the Dataset
Now, we can go through the summary of the train dataset after all those manipulation. Here, we introduce a new column called "Disease_Type" which is as same as the Disease column.
```{r echo=FALSE}
train <- train[,c(1,2,3,4,13,5,6,7,8,9,10,11,12)]
```

```{r}
train$Disease_Type <- as.factor(ifelse(train$Disease == 1, "Contracted", "Healthy"))
dim(train)
str(train)
```

```{r}
```
# Exploratory Data Analysis and Data Visualization
First of all, demonstarting the distribution of all the numeric columns by using histogram. Excluding those binary attributes, some of them are right skewed while some of them do not follow the pattern of nornal distribution. 
```{r warning=FALSE, fig.width=12, fig.height=8, cache=TRUE}
train %>% keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(x = value)) +
  facet_wrap(~key, scales = "free") +
  geom_histogram(bins = 15, fill = "lightblue", color = "white") +
  labs(title = "Show Distribution of Each Numeric Column")
```

```{r}
```
<br>
Before visualizing all the data, we might need to visualize the target variable at first. It is good to see that the target variable is balenced. 
```{r}
barplot(table(train$Disease_Type), col = c("lightblue","pink"), main = "Barplot of Disease", ylab = "Count")
```
<br>
**Stacked Barplots for Gender, Cholesterol, and Glucose**
<br>
Then, using barplots to display the frequency of categorical variables, such as Gender, Cholesterol, and Glucose.
<br>
<br>
* **Gender**: The number of male who contracts the disease is similar with that of male who does not. Likewise, The number of female who contracts the disease is slightly higher than that of female who does not. However, it is easy to find out that the number of female who gets the disease (which is 14,107) is significantly higher than that of male (which is 7,782). Similar pattern could be witnessed from the group of people who does not get the disease. Then, one assumption could be made is that female has a higher chance to get contracted. However, **given that the number of female records in the dataset is higher than that of male, the assumption might not be confirmed.**
<br>
<br>
* **Cholesterol**: The number of people who contracts the disease is similar with that of people who does not. For those who got this disease, the proportion of people who has a normal Cholesterol value is the highest (which is 14,338). For those who has "High" or "Too.High" in Cholesterol value, the number of "Too.High" is higher than that of people who has a high Cholesterol value in the contracted group. On the contrary, the pettern is quit the opposite for the healthy group. Hence, we can make an **assumption that people who has an rather high Cholesterol value tends to contract that disease**.
<br>
<br>
* **Glucose**: For those who get this disease, the proportion of people who has a normal Glucose value is the highest (which is 17,859). Then the number of people who has a rather high Glucose value is slightly higher than that of people who has a high Cholesterol value. Similarly, the number of "Too.High" is higher than that of "High" in contracted group. In the healthy group, the conclusion is different. Then, it seems that **a higher value in Glucose could also contributes to the chance of getting the disease**.
<br>
```{r warning=FALSE, fig.width=18, fig.height=18}
gender <- as.data.frame(table(train$Gender,train$Disease_Type))
colnames(gender) <- c("Gender","Disease","Count")
Gender.plot <- ggplot(gender, aes(x = Disease, y = Count, fill = Gender, label = Count)) +
  geom_bar(stat = "identity") +
  geom_text(size = 5, position = position_stack(vjust = 1.1)) +
  theme_minimal() +
  scale_color_brewer(palette = "Paired")

Cholesterol <- as.data.frame(table(train$Cholesterol, train$Disease_Type))
colnames(Cholesterol) <- c("Cholesterol","Disease","Count")
Cholesterol.plot <- ggplot(Cholesterol, aes(x = Disease, y = Count, fill = Cholesterol, label = Count)) +
  geom_bar(stat = "identity") +
  geom_text(size = 5, position = position_stack(vjust = 1)) +
  theme_minimal() +
  scale_color_brewer(palette = "Paired")

Glucose <- as.data.frame(table(train$Glucose, train$Disease_Type))
colnames(Glucose) <- c("Glucose","Disease","Count")
Glucose.plot <- ggplot(Glucose, aes(x = Disease, y = Count, fill = Glucose, label = Count)) +
  geom_bar(stat = "identity") +
  geom_text(size = 5, position = position_stack(vjust = 1)) +
  theme_minimal() +
  scale_color_brewer(palette = "Paired")

ggarrange(Gender.plot, Cholesterol.plot, Glucose.plot, labels = c("Gender", "Cholesterol", "Glucose"), ncol = 2, nrow = 2)
```
<br> 
<br> 
Next, we might want to further explore the data distribution for contracted group in terms of continuous data type column, such as Age, BMI, Lower Blood Pressure, and Higher Blood Pressure. Here, Age and BMI will be displayed together while the two type of Blood Pressure will be displayed together.
<br>
<br> 
**Barplots for Age and BMI**
<br>
* **Age**: For the healthy group, the distribution does not have certain fix pattern. For the disease contracted group, it seems that the number of people increases with the increases of age. Hence, we can make an assumption that **elder people tends to get this disease**.
<br>
<br>
* **BMI**: To better visualize the plot, we can round the BMI value to whole digit number. For both the healthy and contracted groups, the distributions seem quite similar. However, the **BMI value of healthy people is generally higher than that of contracted patient**.
<br>
```{r warning=FALSE, fig.width=12, fig.height=12}
Age <- as.data.frame(table(train$Age, train$Disease_Type))
colnames(Age) <- c("Age","Disease","Count")
Age.plot <- ggplot(Age, aes(x = Age, y = Count, fill = Disease)) +
  geom_col() + 
  ylim(c(0,1500)) +
  geom_text(aes(label = Count), vjust = -0.5) +
  facet_grid(Disease ~ ., scales = "free") +
  theme_minimal()

bmi <- as.data.frame(table(round(train$BMI,0), train$Disease_Type))
colnames(bmi) <- c("BMI","Disease","Count")
BMI.plot <- ggplot(bmi, aes(x = BMI, y = Count, fill = Disease)) +
  geom_col() + 
  facet_grid(Disease ~ ., scales = "free") +
  ylim(c(0,2800)) +
  geom_text(aes(label = Count), vjust = -0.5) +
  theme_minimal()

ggarrange(Age.plot, BMI.plot, labels = c("Age", "BMI"), ncol = 1, nrow = 2) 
```
<br> 
**Barplots for Blood Pressure**
<br>
* **Low Blood Pressure**: To better visualize the plot, we can use filter to eliminate records with count that is lower than 10. Then, compared with healthy group, **people with higher lower blood pressure seems to have a higher possibility to contract the disease**. Namely, people with the disease has higher cholesterol level.
<br>
<br>
* **High Blood Pressure**: To better visualize the plot, we can use filter to eliminate records with count that is lower than 10. Comparing with the result obtained from the Low.Blood.Pressure barplot, similar conclusion could be made. The conclusion is obvious since the number of people who gets disease is definitly higher than that of people who does not.
<br>
```{r warning=FALSE, fig.width=12, fig.height=10}
Low.Blood <- as.data.frame(table(train$Low.Blood.Pressure, train$Disease_Type))
colnames(Low.Blood) <- c("Low.Blood.Pressure","Disease","Count")
Low.Blood <- Low.Blood %>% filter(Count >= 10)
Low.Blood.plot <- ggplot(Low.Blood, aes(x = Low.Blood.Pressure, y = Count, color = Disease, fill = Disease)) +
  geom_col() +
  facet_grid(Disease ~ ., scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(c(0,13000)) +
  geom_text(aes(label = Count), vjust = -0.5) +
  theme_minimal()

High.Blood <- as.data.frame(table(train$High.Blood.Pressure, train$Disease_Type))
colnames(High.Blood) <- c("High.Blood.Pressure","Disease","Count")
High.Blood <- High.Blood %>% filter(Count >= 10)
High.Blood.plot <- ggplot(High.Blood, aes(x = High.Blood.Pressure, y = Count, color = Disease, fill = Disease)) +
  geom_col() +
  facet_grid(Disease ~ ., scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylim(c(0,13000)) +
  geom_text(aes(label = Count), vjust = -0.5) +
  theme_minimal()

ggarrange(Low.Blood.plot, High.Blood.plot, labels = c("Low.Blood", "High.Blood"), ncol = 1, nrow = 2) 
```
<br> 
**Barplot for Alcohol**
<br>
* **Alcohol**: Summarizing this barplot, we can find that, for people who drinks, the number of contracted  patient is higher than that of healthy patient. On the contrary, for those who do not drink alcohol, the pattern is quit the opposite even though the gap is quit small. If we taking gender into consideration, we can find that the number of contracted female who drinks a lot is higher than that of contracted male who also drinks a lot. Then, **drinking might have a minor influence the disease contract possibility**. 
```{r warning=FALSE, fig.width=12, fig.height=8}
Alcohol <- as.data.frame(table(train$Alcohol, train$Gender, train$Disease_Type))
colnames(Alcohol) <- c("Alcohol","Gender","Disease","Count")
Alcohol$Alcohol <- as.factor(ifelse(Alcohol$Alcohol == 0, "Drink", "Not Drink"))
ggplot(Alcohol, aes(x = Alcohol, y = Count, color = Disease, fill = Disease)) +
  geom_col(stat="identity", position="dodge") +
  facet_grid(Gender ~ ., scales = "free") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme_minimal()
```

```{r}
str(train)
```
## PCA Analysis & Correlation Plot
We can also use PCA techniques and draw correlation plot to discover the relationships among these columns.
<br>
Before continuing that, we might need to convert all the categorical data to numeric data via dummy function.
```{r}
dummies <- train[,sapply(train, is.numeric)]
dmy <- caret::dummyVars("~Gender", data = train, fullRank = T)
dummies <- cbind(dummies, data.frame(predict(dmy, newdata = train)))
dmy <- caret::dummyVars("~Cholesterol", data = train, fullRank = F)
dummies <- cbind(dummies, data.frame(predict(dmy, newdata = train)))
dmy <- caret::dummyVars("~Glucose", data = train, fullRank = F)
dummies <- cbind(dummies, data.frame(predict(dmy, newdata = train)))
```

```{r echo=FALSE}
dummies <- dummies[,c(1,11,2,3,4,5,6,7,8,9,12,13,14,15,16,17,10)]
```
Go through the structure of dummies dataset.
```{r}
str(dummies)
```
Same process for test dataset
```{r}
dummies_test <- test[,sapply(test, is.numeric)]
dmy_test <- caret::dummyVars("~Gender", data = test, fullRank = T)
dummies_test <- cbind(dummies_test, data.frame(predict(dmy_test, newdata = test)))
dmy_test <- caret::dummyVars("~Cholesterol", data = test, fullRank = F)
dummies_test <- cbind(dummies_test, data.frame(predict(dmy_test, newdata = test)))
dmy_test <- caret::dummyVars("~Glucose", data = test, fullRank = F)
dummies_test <- cbind(dummies_test, data.frame(predict(dmy_test, newdata = test)))
```
```{r echo=FALSE}
dummies_test <- dummies_test[,c(2,11,3,4,10,5,6,7,8,9,12,13,14,15,16,17)]
```
<br>
**PCA Analysis**
<br>
We can use PCA analysis to visualize how this data correlate with each other and in which degree this data stays significantly to the dataset.
This is easy for now since we are using the whole dataset, but will require some modifications for model fitting with a train and test set.
```{r fig.width=8, fig.height=8}
# pca analysis
vis_pca <- function(df){
  res.pca <- prcomp(df, scale = TRUE)
  fviz_pca_var(res.pca,
               col.var = "contrib", # Color by contributions to the PC
               gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
               repel = TRUE     # Avoid text overlapping
  )
}
vis_pca(dummies[,-c(17)])
```
<br>
The PCA plot is rather messy. We need to drop columns that have less contribution to the dataset or columns that have similar direction. Then we can get features (or columns) that are significant to the dataset. 
```{r fig.width=8, fig.height=8}
dummies_pca <-dummies[,c(1,5,6,7,8,12,13)]
vis_pca(dummies_pca)
```
<br>
**Correlation Plot**<br>
Then, we can generate a correlation plot to further investigate the relationships amongh those variables.<br>
<br>
After analyzing the plot, there are some obvious correlations here:
1. Weight and BMI has strong positive correlation since BMI derives from this column. <br>
2. High.Blood.Pressure ("Normal") and Low.Blood.Pressure ("Normal") are strongly positively correlated with the other two factors in their variable ("High" and "Too.High"). <br>
3. We can summarise that **Age, Blood pressure (both), and Cholesterol ("High" and "Too.High") have a positive relationship with the disease**. <br>
4. **Weight, BMI, and Glucose are slightly negatively correlated with the disease.** <br>
5. Among those variables, **Blood pressure have a strong relationshio among those features**.
<br> 
<br>
Even though the multicollinearity will theoratically impact the performance of regression models, we will still going to use all of the dummy variable in order to find out the most important feature.
```{r fig.width=7, fig.height=7}
cor_matrix <- cor(dummies[complete.cases(dummies), sapply(dummies, is.numeric)], method = "pearson")
corrplot(cor_matrix, type = "upper")
```

```{r}

```
# Models Construction
Now it is time to establish models. In this case, we will using several Classification Models to fit the dataset since this problem is a classification and regression problem. Each of the model has its own merits and disadvantages. Hence, we need to use several performance evaluation matrix or techniques to assess model performance. Those models are: 
<br>
* Logistic Regression <br>
* Deep Learning <br>
* Naive Bayes Classifier <br>
* KNN or k-Nearest Neighbors <br>
* Random Forest <br>
* Gradient Boosting Machine <br>
* Linear Support Vector Machines <br>
* SVM with RBF Kernal <br>

## Data Sampling
To evaluate the model performance, we need to splite the tagged dataset (which is the train dataset) into two. The sample_train subset will be used for model training and tunning while the sample_test will be used to test predict ability by calculating Accuracy, Specificity, Recall, and ROC value. Besides, since the original train dataset contains at least 40,000 rows of records, we need to sample a subset (which contains 18,000 records) from it in order to reduce the cost of time. <br>
Besides, according to the PCA result, we could remove Gender, Height, Glucose.high, and Cholesterol.high. Removing Glucose.high, and Cholesterol.high is that Glucose.high and Cholesterol.high are positively correlated with Glucose.too.high, and Cholesterol.too.high, giveing that we might need to conduct Regression model, we could eliminate the those two columns to solve multicollinearity issue. Besides, accoring to one-hotspot requirement, you have to eliminate one and keep that as a base line.
```{r}
set.seed(2000)
dummies$Disease.Type <- as.factor(train$Disease)
dummies_sample <- sample_n(dummies[,-c(2,3,11,14,17)], 18000)
# Same process for Test dataset
New_Test <- dummies_test[,-c(2,3,11,14,17)]
```
check the structure again.
```{r}
str(dummies_sample)
```
Since some models are sensitive towards irrelevant attributes & outliers and noisy data, we need to use the **standardized and centuralized dataset**.
<br>
It is important to do Data Pre-Processing within the sample used to evaluate each model, to ensure that the results account for all the variability in the test. If the data set say normalized or standardized before the tuning process, it would have access to additional knowledge (bias) and not give an as accurate estimate of performance on unseen data.
```{r}
pre_process <- preProcess(dummies_sample, method = c("scale", "center"))
dummies_scale <- predict(pre_process, newdata = dummies_sample)

# Same process for Test Dataset
pre_process_test <- preProcess(New_Test, method = c("scale", "center"))
scale_test <- predict(pre_process_test, newdata = New_Test)
```
Next, randomly sample the subset into two. The sample_train contains 70% of the original subset data while the sample_test contains the rest of them. 
```{r}
train_index <- createDataPartition(dummies_scale$Disease.Type, p = 0.7, list = FALSE)
sample_train <- dummies_scale[train_index,]
sample_test <- dummies_scale[-train_index,]
```
### Logistic Regression
Logistic regression is one of the most classic and powerful analysis tool in data analysis. However, it still requires severl pre-requisites, or the so-called assumptions, to be satiated, otherwise the result will be tortured. Hence, people introduced a penalty system to regression which are Lasso- and Ridge- logistic regression. In this project, we will, at first, conduct a normal logistic regression and then, using caret package, conduct a lasso and ridge logistic regression.
```{r}

```
**Logistic Regression by Using Step Function**
<br>
We can use the setp function to conduct setpwise logistic regression. This model will be viwed as the baseline model of logistic regression experiments. Here we can see that the final optimal result is listed below. If we look at the z-value, it is clear that Weight and Alchohol are not significant since their values are higher than 0.05. In this case, we should have eliminate these two. However, considering that we will conduct lasso and ridge logistic regression later, we could keep them and see what will happen. Now let's look at the deviance residuals, we know that this dataset is slightly right skewed.
```{r results='hide'}
logistic.all <- stats::step(glm(Disease.Type~.,data=sample_test,family = binomial()))
```
Then, we can use summary to check the best model. In this final model, Age, BMI, High.Blood.Pressure, Low.Blood.Pressure, Smoke, Alcohol, Exercise, Cholesterol.normal, Cholesterol.too.high, and Glucose.too.high have been selected.All of those elements are significant in p-value.
```{r}
summary(logistic.all)
```
<br>
Then, we can use anova() function to test the authenticity of the final model.  It's good to see that all those variables are significant in chi-test.
```{r}
varImp(logistic.all)
anova(logistic.all, test="Chisq")
```
<br>
**Lasso logistic regression & Ridge Logistic Regression**
<br>
LASSO is a penalized regression approach that estimates the regression coefficients by maximizing the log-likelihood function (or the sum of squared residuals) with the constraint that the sum of the absolute values of the regression coefficients. Lasso has the ability to automatically deletes unnecessary covariates. 
<br>
First of all, we need to assign the target variables of train dataset to y_train while assign the rest variables to x_train by using as.matrix function. Then, we can use cv.glmnet function to get the best lambda (which has the lowest error rate) of lasso and ridge logistic regression respectively. Now we get the range of lambda for this sample_train dataset.
```{r}
y_train <- as.factor(sample_train[,13])
x_train <- as.matrix(sample_train[,1:12])

cv_lasso <- cv.glmnet(x_train, y_train, type.measure="class", alpha = 1, nfolds=10, family="binomial")
cv_lasso$lambda.min
cv_ridge <- cv.glmnet(x_train, y_train, type.measure="class", alpha = 0, nfolds=10, family="binomial")
cv_ridge$lambda.min
```
Here, we can use the combination of "glmnet" and "train" function equippted by caret package. Based on the lambda range we obtained above, we can set the tunGrid as "0 to 0.05 while each step is 0.0001".
```{r eval=FALSE}
time_logistic <- Sys.time()
model_logistic <- train(Disease.Type ~ .,
                  data = sample_train,
                  method = "glmnet",
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
                  tuneGrid = expand.grid(alpha=0:1, lambda=seq(0, 0.05, by = 0.0001)))
time_logistic <- Sys.time() - time_logistic
save(model_logistic, file = "model_logistic.RData")
save(time_logistic, file = "time_logistic.RData")
```

```{r echo=FALSE}
load("model_logistic.RData")
load("time_logistic.RData")
```
Next, we can visualize the output.
```{r}
plot(model_logistic)
resampleHist(model_logistic)
ggplot(varImp(model_logistic), main = "Variable Importance with Logistic Regression")
```
<br>
Combining the results of top 5 cases with the highest Accuracy rates with the results displayed above, we can conclude that **setting alpha value as "1" and setting lambda as "0.001"** is most beneficial for the RF model in this dataset.
```{r}
model_logistic$results %>% top_n(5, wt = Accuracy) %>% arrange(desc(Accuracy))
```
Storing the Hyperparameters.
```{r}
parameter_LR <- c("Alpha = 1; Lambda = 0.001")
```
**Performance Display**
<br>
Display the model performance. The accuracy rate is 0.724. The specificity (which is 0.6718) is satisfactory. The value of sensitivity is 0.7766 which is relatievly high compared with specificity. This indicats that Lasso Logistic Regression has a better performance in detecting potential patient. 
```{r}
predict_logistic <- predict(model_logistic, newdata = sample_test[,-c(14)], type = "prob")
Matrix_logistic <- confusionMatrix(as.factor(ifelse(predict_logistic$`1`> 0.5, 1, 0)), as.factor(sample_test$Disease.Type))
Matrix_logistic
```
**ROC line**
<br>
Plotting Receiver Operating Characteristics (ROC) curve and displaying the value of Area Under the Curve (AUC).
```{r error=FALSE}
roc <- roc(sample_test$Disease.Type, predict_logistic$`1`,levels = c(0, 1), direction = "<")
plot(roc)
auc(roc)
```
Storing the performance index into Per_names data frame.
```{r}
Per_names <- as.data.frame(c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall", "ROC", "Time"))
rownames(Per_names) <- c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall", "ROC", "Time")
colnames(Per_names) <- "LR"
Per_names$LR <- c(as.numeric(Matrix_logistic$overall[1]), as.numeric(Matrix_logistic$byClass[1]), as.numeric(Matrix_logistic$byClass[2]), as.numeric(Matrix_logistic$byClass[5]), as.numeric(Matrix_logistic$byClass[6]), as.numeric(roc$auc), time_logistic/60)
```
Utilizing the NBC model to predict the classification result of test data. Storing these results to the result data frame.
```{r warning=FALSE, cache=TRUE}
result <- data.frame()[1:dim(test)[1],]
result$ID <- 1:dim(test)[1]
result$LR <- predict(model_logistic, newdata = scale_test, type = "raw")
rownames(result) <- 1:dim(test)[1]
```

### Artificial Neural Networks / Deep Learning Models
Artificial Neural Networks are best when the data is one-hot encoded, scaled and centered. In addition, other transformations may be beneficial as well to make relationships easier for the algorithm to identify. In this case, we have already dummified, centered and scaled the dataset.<br><br>

Firstly, initialize a sequential model: The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack of layers. And then, we can add layers to the sequential model, which consists of input layer, hidden layer(s), and output layer.<br>
* **Hidden Layers**: form the neural network nodes that enable non-linear activation using weights. The hidden layers are created using layer_dense().<br>
The common arguments used here:<br>
  1. **units**: a positive integer representing the dimension of the output space of the layer;<br>
  2. **activation**: the name of the activation function to use. Available options are elu, softmax, selu, softplus, relu, sigmoid, tanh, lieanr, etc;<br>
  3. **bias_initializer**: specifies the initializer for the bias vector. Available options are Zeros, Ones, Constant, identity, etc;<br>
  4. **weights**: specifies the initial weights for layer;<br>
  5. **input_shape**: specifies the dimentionality of the input in the first layer of the model.<br>
* **Dropout Layers**: are used to control overfitting. This eliminates weights below a cutoff threshold to prevent low weights from overfitting the layers.
<br>
* **Output Layer**: specifies the shape of the output and the method of assimilating the learned information. The output layer is applied using the layer_dense(). .
<br>
<br>
Hence, the whole process is:<br>
1. **Defining the model**: providing the *number of layers* and neurons for each layer; and *adding any regularization technique* to avoid overfitting<br>
2. **Compilng the model**: defining the *optimizer*; providing the *loss function*; and identifying the *metrics*<br>
  The common arguments used here:<br>
     **optimizer**: it specifies the optimization technique (e.g., SGD, Adam, etc.) used for weights and biases updating. <br>
     **loss**: it specifies the name of objective function or objective function (e.g., binary_crossentropy, categorical_crossentropy, etc.).<br>
     **metrics**: it specifies the list of metrics to be evaluated by the model during training and testing (e.g., binary_accuracy, etc.). It is also worth mentioning that more than        one metrics can be used at the same time when training and testing a model. <br>
3. **Fitting the model**: introducing the *number of batches*; the *number of epochs*; and the validation split<br>
  Full list of arguments for fit() function are:<br>
     **batch_size**: The number of samples per gradient updates. The default is 32. <br>
     **epochs**: The number of epochs to train the model;<br>
     **callbacks**: List of callbacks to be called during training;<br>
     **validation_split**: Float between 0 and 1. The model sets apart the last fraction of the x and y data provided and use it as a validation set.<br>
     **validation_data**: Data provided to evaluate the model metrics and loss at the end of each epoch. If provided, it will override validation_split.<br>
4 .**Evaluating the model**: evaluating the model on the test data set; and demonstrating the plots.<br>
5. **Predicting the classes/probabilities** for the test data set.<br>
<br>
Now, let's us adjust the datasets to fit the ANN model. We use the recipe() function to implement our preprocessing steps (includes centered and scaled data).
```{r}
rec_obj <- recipe(Disease.Type ~ ., data = sample_train) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = sample_train)
```
Here, we can use bake() function in the recipt package to any data set, and it processes the data following our recipe steps. We’ll apply to oursample_train and sample_test to convert from raw data to a machine learning dataset. Then, we need to store the target values as y_train and y_test, which are needed for modeling our ANN. 
```{r}
x_train <- bake(rec_obj, new_data = sample_train[,c(1:12)])
y_train <- ifelse(pull(sample_train, Disease.Type) == 1, 1, 0)

x_test <- bake(rec_obj, new_data = sample_test[,c(1:12)])
y_test <- ifelse(pull(sample_test, Disease.Type) == 1, 1, 0)
```
<br>
**Perceptron**<br>
Here, let's discuss the choices of hyperparameters in single layer perceptron.<br> 
***For the output layer:*** Becasue we only have one output layer here, we need to set the unit as '1' since this is a binary classification project. For binary values, the shape should be units = 1. We set the kernel_initializer = "uniform" and the activation = "sigmoid" (common for binary classification). The input_shape should be 12, which is the number of variables inside the dataset. Besides, we can also add a kernel_regularizer to regiularize the output in case of overfitting. <br>
***For compile process:*** The optimizer will be "adam" which learns the fastest (compared with SGD) and generally has a relatively wide range of successful learning rates. As for the loss function, we might use 'binary_crossentropy' which is the default loss function to use for binary classification problems.<br>
***For fit process:*** In the single layer perceptron, it seems that a small value (such as '20') will generate a relatively higher accuracy rate compared with a large value (such as '1000'). In general, batch size of 32 is a good starting point, and you should also try with 64, 128, and 256. The epochs will be set as 30. Besides, we can also use callback_early_stopping() function to monitor a certain value and urge the iteration to stop if that value does not change anymore. For example, callback_early_stopping(monitor='loss', patience=5) will monitor the 'loss' value. But in this project, we will not use this function since we want to thoroughly explore the data and conduct the experiments.<br>
```{r warning=FALSE, message=FALSE, cache=TRUE, results='hide'}
time_model_ann0 <- Sys.time()
model_ann0 <- keras_model_sequential()
model_ann0 %>%
  layer_dense(units = 1, kernel_initializer = "uniform", activation = "sigmoid", input_shape = 12, kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  compile(optimizer = "adam", loss = "binary_crossentropy", metrics = c("acc"))

history <- fit(object = model_ann0, x = as.matrix(x_train), y = y_train, batch_size = 256, epochs = 30, validation_split = 0.3)
time_model_ann0 <- Sys.time() - time_model_ann0
```
Storing the Hyperparameters.
```{r}
parameter_ANN0 <- c("batch_size = 256, epochs = 30, validation_split = 0.3")
```
Then, we can visualize the model.
```{r error=FALSE, message=FALSE}
plot(history)
```
After geting the model, we can use the model to fit x_test dataset (which is the sample_test) and check the predicted result.
```{r}
yhat_keras_class <- predict_classes(object = model_ann0, x = as.matrix(x_test)) %>% as.vector()
yhat_keras_prob  <- predict_proba(object = model_ann0, x = as.matrix(x_test)) %>% as.vector()

estimates_keras <- tibble(
  truth      = as.factor(y_test) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class) %>% fct_recode(yes = "1", no = "0"),
  class_prob = yhat_keras_prob
)
```
Here is the confusion matrix of ANN0. 
```{r}
options(yardstick.event_first = FALSE)
estimates_keras %>% conf_mat(truth, estimate)
```
To better save the model evaluation result to the Per_names dataframe, we can create a function to warp the processes. And then add the result vector to the dataframe.
```{r}
getResult <- function(estimates_keras, time){
  Acc <- estimates_keras %>% metrics(truth, estimate)
  Specificity <-estimates_keras %>% specificity(truth, estimate)
  Sensitivity <-estimates_keras %>% sensitivity(truth, estimate)
  Precision <- estimates_keras %>% precision(truth, estimate)
  Recall <-estimates_keras %>% recall(truth, estimate)
  Auc <- estimates_keras %>% roc_auc(truth, class_prob)
  Result <- c(as.numeric(Acc$.estimate[1]), as.numeric(Sensitivity$.estimate), as.numeric(Specificity$.estimate), as.numeric(Precision$.estimate),
                     as.numeric(Recall$.estimate), as.numeric(Auc$.estimate), as.numeric(time))
  return(Result)
}
Modelresult <- getResult(estimates_keras, time_model_ann0/60)
Per_names$ANN0 <- Modelresult
```
Utilizing the NBC model to predict the classification result of test data. Storing these results to the result data frame.
```{r warning=FALSE, cache=TRUE}
result$ANN0 <- predict_classes(object = model_ann0, x = as.matrix(scale_test)) %>% as.vector()
```
<br>

**MLP - One Layer**<br>
***For the first layer:*** The setting of kernel_initializer, kernel_regularizer, and input_shape are the same as we set in ANN0. Here, I set the units as 12 since there are 12 variables inside the dataset. In this way, the number of output of the first layer will be 12 which is correpsonding to that of input_shape. For the activition method, 'relu' has been chosen  Then, we want the dropout layer to remove weights below 5%. Then, process like output layer will be conducted. The last process if compile the result, which is similar with ANN0.<br>
***For dropout layer:*** We use the layer_dropout() function add two drop out layers with rate = 0.05 to remove weights below 5%.<br>
```{r warning=FALSE, message=FALSE, cache=TRUE}
time_model_ann1 <- Sys.time()
model_ann1 <- keras_model_sequential()
model_ann1 %>%
  layer_dense(units = 12, kernel_initializer = "uniform", activation = "relu", input_shape = 12, kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dropout(rate = 0.05) %>% 
  layer_dense(units = 1, kernel_initializer = "uniform", activation = "sigmoid") %>% 
  compile(optimizer = "adam", loss = "binary_crossentropy", metrics = c("acc"))
```
Using model_ann1 to fit the sample_train dataset (without target variable).
```{r results='hide'}
history <- fit(object = model_ann1, x = as.matrix(x_train), y = y_train, batch_size = 256, epochs = 30, validation_split = 0.3)
time_model_ann1 <- Sys.time() - time_model_ann1
```
Storing the Hyperparameters.
```{r}
parameter_ANN1 <- c("units = 12, kernel_initializer = uniform, activation = relu, input_shape = 12, regularizer_l2(l = 0.001), layer_dropout = 0.05, batch_size = 256, epochs = 30, validation_split = 0.3")
```
Then, we can visualize the model. The pattern of accuracy line is similar with that of accuracy line we ploted in ANN0. 
```{r error=FALSE, message=FALSE}
plot(history)
```
<br>
Again, using the model to fit x_test dataset and get the predicted result.
```{r}
yhat_keras_class <- predict_classes(object = model_ann1, x = as.matrix(x_test)) %>% as.vector()
yhat_keras_prob  <- predict_proba(object = model_ann1, x = as.matrix(x_test)) %>% as.vector()

estimates_keras <- tibble(
  truth      = as.factor(y_test) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class) %>% fct_recode(yes = "1", no = "0"),
  class_prob = yhat_keras_prob
)
```
Here is the confusion matrix of ANN1. For some reseaons, the reuslt is similar as we obtained from the ANN0's confusion matrix. The Negative Flase decreases slighlt while the Negative True increases slighlt. We can assumed that the first layer did not bring benefits to the model pridiction performance. 
```{r}
estimates_keras %>% conf_mat(truth, estimate)
```
To better save the model evaluation result to the Per_names dataframe, we can create a function to warp the processes. And then add the result vector to the dataframe.
```{r}
Modelresult <- getResult(estimates_keras, time_model_ann1/60)
Per_names$ANN1 <- Modelresult
```
Utilizing the NBC model to predict the classification result of test data. Storing these results to the result data frame.
```{r warning=FALSE, cache=TRUE}
result$ANN1 <- predict_classes(object = model_ann1, x = as.matrix(scale_test)) %>% as.vector()
```
<br>
**MLP - Two Layers**
<br>
***For the first layer:***  The setting of kernel_initializer, kernel_regularizer, and input_shape are the same as we set in ANN1. <br>
***For the second layer:***  The setting are also the same except input_shape and rergularizer has been removed while the second dropout rate has been reduced to 0.01.<br>
Using model_ann2 to fit the sample_train dataset (without target variable).
<br>
```{r warning=FALSE, message=FALSE, cache=TRUE}
time_model_ann2 <- Sys.time()
model_ann2 <- keras_model_sequential()
model_ann2 %>%
  layer_dense(units = 12, kernel_initializer = "uniform", activation = "relu", input_shape = 12, kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dropout(rate = 0.05) %>%         
  layer_dense(units = 12, kernel_initializer = "uniform", activation = "relu") %>%
  layer_dropout(rate = 0.01) %>% 
  layer_dense(units = 1, kernel_initializer = "uniform", activation = "sigmoid") %>% 
  compile(optimizer = "adam", loss = "binary_crossentropy", metrics = c("acc"))
```

```{r results='hide'}
history <- fit(object = model_ann2, x = as.matrix(x_train), y = y_train, batch_size = 256, epochs = 30, validation_split = 0.3)
time_model_ann2 <- Sys.time() - time_model_ann2
```
Storing the Hyperparameters.
```{r}
parameter_ANN2 <- c("layer1: units = 12, kernel_initializer = uniform, activation = relu, input_shape = 12, regularizer_l2(l = 0.001); layer_dropout1 = 0.05; Layer2: units = 12, kernel_initializer = uniform, activation = relu; layer_dropout2 = 0.01, batch_size = 256, epochs = 30, validation_split = 0.3")
```
Then, we can visualize the model. 
```{r error=FALSE, message=FALSE}
plot(history)
```
<br>
Again, using the model to fit x_test dataset and get the predicted result.
```{r}
yhat_keras_class <- predict_classes(object = model_ann2, x = as.matrix(x_test)) %>% as.vector()
yhat_keras_prob  <- predict_proba(object = model_ann2, x = as.matrix(x_test)) %>% as.vector()

estimates_keras <- tibble(
  truth      = as.factor(y_test) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class) %>% fct_recode(yes = "1", no = "0"),
  class_prob = yhat_keras_prob
)
```
Here is the confusion matrix of ANN1. For some reseaons, the reuslt is similar as we obtained from the ANN0's confusion matrix. The Negative Flase decreases slighlt while the Negative True increases slighlt. We can assumed that the first layer did not bring benefits to the model pridiction performance. 
```{r}
estimates_keras %>% conf_mat(truth, estimate)
```
To better save the model evaluation result to the Per_names dataframe, we can create a function to warp the processes. And then add the result vector to the dataframe.
```{r}
Modelresult <- getResult(estimates_keras, time_model_ann2/60)
Per_names$ANN2 <- Modelresult
```
Utilizing the NBC model to predict the classification result of test data. Storing these results to the result data frame.
```{r warning=FALSE, cache=TRUE}
result$ANN2 <- predict_classes(object = model_ann2, x = as.matrix(scale_test)) %>% as.vector()
```

### Naive Bayes Classifer
**Model Tunning**
<br>
We can tune the few hyperparameters that a naïve Bayes model has:<br>
* **usekernel**: allows us to use a kernel density estimate for continuous variables versus a guassian density estimate. <br>
* **adjust**: allows us to adjust the bandwidth of the kernel density (larger numbers mean more flexible density estimate). <br>
* **fL**: allows us to incorporate the Laplace smoother.
<br>
It is worthwhile to point out that we use the preProc function to normalize with Box Cox and reducing with PCA.
```{r eval=FALSE}
time_NBC <- Sys.time()
model_nb <- train(Disease.Type ~.,
                  data = sample_train,
                  method = 'nb',
                  preProc = c("BoxCox", "pca"),
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
                  tuneGrid = expand.grid(fL = 0:5, usekernel = c(TRUE, FALSE), adjust = seq(0, 5, by=1)))
time_NBC <- Sys.time() - time_NBC
save(model_nb, file = "model_nbc.RData")
save(time_NBC, file = "time_NBC.RData")
```

```{r echo=FALSE}
load("model_nbc.RData")
load("time_NBC.RData")
```
Visualize the model.
```{r}
plot(model_nb)
resampleHist(model_nb)
ggplot(varImp(model_nb), main = "Variable Importance with NBC")
```
<br>
Combining the results of top 5 cases with the highest Accuracy rates with the results displayed above, we can conclude that the combination of **setting usekernal as "TRUE" (density estimation is being used rather then normal density), setting adjust as '1', and setting fL as "0" (no laplace smotthing effect)** is most beneficial for NBC model in this dataset. 
<br>
```{r}
model_nb$results %>% top_n(5, wt = Accuracy) %>% arrange(desc(Accuracy))
```
Storing the Hyperparameters.
```{r}
parameter_NBC <- c("userKernel = 'True', Ajust = 1; fL = 0")
```
**Performance Display**
<br>
Display the model performance. The accuracy rate is 0.7033 which fine but is not very satisfactory. The accuracy rate is within the confidence Interval, implying that the result is trustful. Besides, the sensitivity (0.7178) is higher than specificity (which is 0.6888).
```{r warning=FALSE, message=FALSE}
predict_nb <- predict(model_nb, newdata = sample_test[,-c(13)], type = "prob")
Matrix_nb <- confusionMatrix(as.factor(ifelse(predict_nb$`1`> 0.5, 1, 0)), as.factor(sample_test$Disease.Type))
Matrix_nb
```
**ROC line**
<br>
Plotting Receiver Operating Characteristics (ROC) curve and displaying the value of Area Under the Curve (AUC).
```{r}
roc <- roc(sample_test$Disease.Type, predict_nb$`1`,levels = c(0, 1), direction = "<")
plot(roc)
auc(roc)
```
Storing the performance index into Per_names data frame.
```{r}
Per_names$NBC <- c(as.numeric(Matrix_nb$overall[1]), as.numeric(Matrix_nb$byClass[1]), as.numeric(Matrix_nb$byClass[2]), as.numeric(Matrix_nb$byClass[5]), as.numeric(Matrix_nb$byClass[6]), as.numeric(roc$auc), as.numeric(time_NBC))
```

### K Nearest Neighbor
**Model Tunning**<br>
We can tune the few hyperparameters that a knn model has: <br>
* **k**: Number of neighbors (K). A small value for K provides the most flexible fit, which will have low bias but high variance. Larger values of K will have smoother decision boundaries which means lower variance but increased bias.
```{r eval=FALSE}
time_KNN <- Sys.time()
model_KNN <- train(Disease.Type ~ .,
                   data = sample_train,
                   method = "knn",
                   trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
                   tuneGrid = data.frame(k = seq(20:80)))
time_KNN <- Sys.time() - time_KNN
save(model_KNN, file = "model_KNN.RData")
save(time_KNN, file = "time_KNN.RData")
```

```{r echo=FALSE}
load("model_KNN.RData")
load("time_KNN.RData")
```
Visualize the model.
```{r}
plot(model_KNN)
resampleHist(model_KNN)
ggplot(varImp(model_KNN), main = "Variable Importance with KNN")
```
<br>
Combining the results of top 5 cases with the highest Accuracy rates with the results displayed above, we can conclude that **setting K value as "53"** is most beneficial for the KNN model in this dataset. The Accuracy plot shows a general trends in the increase in performance with K value and that the larger value of k is probably preferred (range between 20 to 80).
```{r}
model_KNN$results %>% top_n(5, wt = Accuracy) %>% arrange(desc(Accuracy))
```
Storing the Hyperparameters.
```{r}
parameter_KNN <- c("K = 53")
```
**Performance Display**
<br>
Display the model performance. The accuracy rate is 0.7229 which is slightly higher than that of NBC model. The specificity (which is  0.6781) is lower than the value of NBC model. Compared with the sensitivity value of NBC model, the value of KNN (0.7680) is significantly higher, indicating that KNN has a better performance in detecting every possible patient. 
```{r}
predict_KNN <- predict(model_KNN, newdata = sample_test[,-c(13)], type = "prob")
Matrix_KNN <- confusionMatrix(as.factor(ifelse(predict_KNN$`1`> 0.5, 1, 0)), as.factor(sample_test$Disease.Type))
Matrix_KNN
```
**ROC line**
<br>
Plotting Receiver Operating Characteristics (ROC) curve and displaying the value of Area Under the Curve (AUC).
```{r}
roc <- roc(sample_test$Disease.Type, predict_KNN$`1`, levels = c(0, 1), direction = "<")
plot(roc)
auc(roc)
```
Storing the performance index into Per_names data frame.
```{r}
Per_names$KNN <- c(as.numeric(Matrix_KNN$overall[1]), as.numeric(Matrix_KNN$byClass[1]), as.numeric(Matrix_KNN$byClass[2]), as.numeric(Matrix_KNN$byClass[5]), as.numeric(Matrix_KNN$byClass[6]), as.numeric(roc$auc), as.numeric(time_KNN))
```

### Random Forest
**Model Tunning**
<br>
We can tune the few hyperparameters that a RF model has:<br>
* **ntree**: the number of trees to grow. Larger the tree, it will be more computationally expensive to build models.<br>
* **mtry**: how many variables we should select at a node split. The default value is p/3 for regression and sqrt(p) for classification. We should always try to avoid using smaller values of mtry to avoid overfitting.<br>
* **nodesize**: how many observations we want in the terminal nodes. This parameter is directly related to tree depth. Higher the number, lower the tree depth. With lower tree depth, the tree might even fail to recognize useful signals from the data. 
<br>
```{r eval=FALSE}
time_RF <- Sys.time()
model_RF <- train(Disease.Type ~ .,
                  data = sample_train,
                  method = "rf",
                  ntree = 1500,
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
                  tuneGrid = expand.grid(mtry = 1:5))
time_RF <- Sys.time() - time_RF
save(model_RF, file = "model_RF.RData")
save(time_RF, file = "time_RF.RData")
```

```{r echo=FALSE}
load("model_RF.RData")
load("time_RF.RData")
```
Visualize the model.
```{r}
plot(model_RF)
resampleHist(model_RF)
ggplot(varImp(model_RF), main = "Variable Importance with RF")
```
<br>
Combining the results of top 5 cases with the highest Accuracy rates with the results displayed above, we can conclude that **setting mtry value as "2" and setting ntree as "1500"** is most beneficial for the RF model in this dataset.
```{r}
model_RF$results %>% top_n(5, wt = Accuracy) %>% arrange(desc(Accuracy))
```
Storing the Hyperparameters.
```{r}
parameter_RF <- c("mtry = 2 and;ntree = 1500")
```
**Performance Display**
<br>
After tuning, we have achieved an overall accuracy of 0.7268, which is the highest up to now. The sensitivity is 0.7933, which is also the highest among the given three models. The specificity (0.6608), however, is lowest among those models.
```{r}
predict_RF <- predict(model_RF, newdata = sample_test[,-c(13)], type = "prob")
Matrix_RF <- confusionMatrix(as.factor(ifelse(predict_RF$`1`> 0.5, 1, 0)), as.factor(sample_test$Disease.Type))
Matrix_RF
```
**ROC line**
<br>
Plotting Receiver Operating Characteristics (ROC) curve and displaying the value of Area Under the Curve (AUC).
```{r}
roc <- roc(sample_test$Disease.Type, predict_RF$`1`, levels = c(0, 1), direction = "<")
plot(roc)
auc(roc)
```
Storing the performance index into Per_names data frame.
```{r}
Per_names$RF <- c(as.numeric(Matrix_RF$overall[1]), as.numeric(Matrix_RF$byClass[1]), as.numeric(Matrix_RF$byClass[2]), as.numeric(Matrix_RF$byClass[5]), as.numeric(Matrix_RF$byClass[6]), as.numeric(roc$auc), as.numeric(time_RF))
```

### Gradient Boosting Machine
**Model Tunning**
<br>
We can tune the few hyperparameters that a XGBoost model has:<br>
* **nrounds**: The max number of iterations. <br>
* **eta**: Step size of each boosting step. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.<br>
* **max_depth**: Maximum depth of the tree. Increasing this value will make the model more complex and more likely to overfit. <br>
* **gamma**: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.<br>
* **min_child_weight**: Minimum sum of instance weight (hessian) needed in a child. The larger min_child_weight is, the more conservative the algorithm will be.<br>
* **subsample**: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.<br>
* etc.
<br>
```{r eval=FALSE}
time_XGBoost <- Sys.time()
model_XGB <- train(Disease.Type ~ .,
                   data = sample_train,
                   method = "xgbTree",
                   trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
                   tuneGrid = expand.grid(nrounds = c(25,50,100,150,200,250,300),
                                          max_depth = c(2,3,5,7,9),
                                          colsample_bytree = seq(0.1, 1, length.out = 5),
                                          eta = 0.1,
                                          gamma=0,
                                          min_child_weight = 1,
                                          subsample = 1))
time_XGBoost <- Sys.time() - time_XGBoost
save(model_XGB, file = "model_XGB.RData")
save(time_XGBoost, file = "time_XGBoost.RData")
```

```{r echo=FALSE}
load("model_XGB.RData")
load("time_XGBoost.RData")
```
Visualize the model.
```{r}
plot(model_XGB)
resampleHist(model_XGB)
ggplot(varImp(model_XGB), main = "Variable Importance with XGB")
```
<br>
Combining the results of top 5 cases with the highest Accuracy rates with the results displayed above, we can conclude that the combination of **setting nrounds = "100", max_depth = "2", eta = "0.1", gamma = "0", colsample_bytree = "0.550", min_child_weight = "1", and subsample = "1"** is most beneficial for the XGBoost model in this dataset.
```{r}
model_XGB$results %>% top_n(5, wt = Accuracy) %>% arrange(desc(Accuracy))
```
Storing the Hyperparameters.
```{r}
parameter_XGB <- c("nrounds = 100; max_depth = 2; eta = 0.1; gamma = 0; colsample_bytree = 0.550; min_child_weight = 1; and subsample = 1")
```
**Performance Display**
<br>
After tuning, we have achieved an overall accuracy of 0.7292, which is slightly higher than that of RF. The sensitivity is 0.7680, which is the second highest. The specificity (which is 0.6907), is the highest up to now.
```{r}
predict_XGB <- predict(model_XGB, newdata = sample_test[,-c(13)], type = "prob")
Matrix_XGB <- confusionMatrix(as.factor(ifelse(predict_XGB$`1`> 0.5, 1, 0)), as.factor(sample_test$Disease.Type))
Matrix_XGB
```
**ROC line**
<br>
Plotting Receiver Operating Characteristics (ROC) curve and displaying the value of Area Under the Curve (AUC).
```{r}
roc <- roc(sample_test$Disease.Type, predict_XGB$`1`, levels = c(0, 1), direction = "<")
plot(roc)
auc(roc)
```
Storing the performance index into Per_names data frame.
```{r}
Per_names$XGB <- c(as.numeric(Matrix_XGB$overall[1]), as.numeric(Matrix_XGB$byClass[1]), as.numeric(Matrix_XGB$byClass[2]), as.numeric(Matrix_XGB$byClass[5]), as.numeric(Matrix_XGB$byClass[6]), as.numeric(roc$auc), as.numeric(time_XGBoost))
```

### Linear Support Vector Machine
**Model Tunning**
<br>
We can tune the few hyperparameters that a RF model has:<br>
* **C**: The Penalty parameter C of the error term. A higher C might cause lower bias but risk of overfitting; A lower C might cause higher bias but lower variance.<br>
* **kernel**: Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable.<br>
* degree: Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.<br>
* gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features will be used instead.<br>
* class_weight: Set the parameter C of class i to class_weight[i]C for SVC. If not given, all classes are supposed to have weight one.<br>
* etc.<br>
```{r eval=FALSE}
time_SVM_Linear <- Sys.time()
model_svm_linear <- train(Disease.Type ~ .,
                          data = sample_train,
                          method = "svmLinear",
                          trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
                          tuneGrid = expand.grid(C = seq(0.05, 1, 0.05)))
time_SVM_Linear <- Sys.time() - time_SVM_Linear
save(model_svm_linear, file = "model_svm_linear.RData")
save(time_SVM_Linear, file = "time_SVM_Linear.RData")
```

```{r echo=FALSE}
load("model_svm_linear.RData")
load("time_SVM_Linear.RData")
```
Visualize the model.
```{r}
plot(model_svm_linear)
resampleHist(model_svm_linear)
ggplot(varImp(model_svm_linear), main = "Variable Importance with Linear SVM")
```
<br>
Combining the results of top 5 cases with the highest Accuracy rates with the results displayed above, we can conclude that **setting C as "0.05"** is most beneficial for the XGBoost model in this dataset. According to the Accuracy Plot, the Accuracy decrease with the increase of the cost value. 
```{r}
model_svm_linear$results %>% top_n(5, wt = Accuracy) %>% arrange(desc(Accuracy))
```
Storing the Hyperparameters.
```{r}
parameter_svm_linear <- c("C = 0.05")
```
**Performance Display**
<br>
After tuning, we have achieved an overall accuracy of 0.7222. The sensitivity is 0.8078, which is the highest up to now. The specificity (which is 0.6371), however, is the lowest among those models.
```{r}
predict_svm_linear <- predict(model_svm_linear, newdata = sample_test[,-c(13)], type = "raw")
Matrix_svm_linear <- confusionMatrix(as.factor(predict_svm_linear), as.factor(sample_test$Disease.Type))
Matrix_svm_linear
```
**ROC line** <br>
Normally, one cannot generate a plotting Receiver Operating Characteristics (ROC) curve for SVM Model.
<br>
Storing the performance index into Per_names data frame.
```{r}
Per_names$Linear_SVM <- c(as.numeric(Matrix_svm_linear$overall[1]), as.numeric(Matrix_svm_linear$byClass[1]), as.numeric(Matrix_svm_linear$byClass[2]), as.numeric(Matrix_svm_linear$byClass[5]), as.numeric(Matrix_svm_linear$byClass[6]), "Nan", as.numeric(time_SVM_Linear))
```

### Support Vector Machine with Non-linear Kernel: RBF
Here, for this SVM model with RBF Kernel, we only consider hyper-parameter "C" and "Sigma" (which is also called gamma). The default gamma value should be 1/k (k is the number of features inside the dataset). This hyper-parameter decides the distribution of data once the data have been projected. Generally, a higher gamma might cause a smaller number of support vector generated by the model, which directly influence the computational speed.
```{r eval=FALSE}
time_SVM_RBF <- Sys.time()
model_svm_rbf <- train(Disease.Type ~ .,
                       data = sample_train,
                       method = "svmRadial",
                       tuneGrid = expand.grid(sigma = seq(0, 0.5, 0.1), C = seq(0.05, 0.5, 0.1)),
                       trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3))
time_SVM_RBF <- Sys.time() - time_SVM_RBF
save(model_svm_rbf, file = "model_svm_rbf.RData")
save(time_SVM_RBF, file = "time_SVM_RBF.RData")
```

```{r echo=FALSE}
load("model_svm_rbf.RData")
load("time_SVM_RBF.RData")
```
Visualize the model.
```{r}
plot(model_svm_rbf)
resampleHist(model_svm_rbf)
ggplot(varImp(model_svm_rbf, main = "Variable Importance with model_svm_rbf"))
```
<br>
Combining the results of top 5 cases with the highest Accuracy rates with the results displayed above, we can conclude that the combination of setting **sigma = 0.1 and C = 0.45** is most beneficial for the XGBoost model in this dataset. 
```{r}
model_svm_rbf$results %>% top_n(5, wt = Accuracy) %>% arrange(desc(Accuracy))
```
Storing the Hyperparameters.
```{r}
parameter_svm_rbf <- c("C = 0.45 and sigma = 0.1")
```
**Performance Display**
<br>
After tuning, we have achieved an overall accuracy of 0.7277, which is a quit normal performance compared with others'. The sensitivity is 0.7836, which is slightly higher than KNN's. The specificity is 0.6722, which is the highest among those 6 models.
```{r}
predict_svm_rbf <- predict(model_svm_rbf, newdata = sample_test[,-c(13)], type = "raw")
Matrix_svm_rbf <- confusionMatrix(as.factor(predict_svm_rbf), as.factor(sample_test$Disease.Type))
Matrix_svm_rbf
```
**ROC line**
<br>
Normally, one cannot generate a plotting Receiver Operating Characteristics (ROC) curve for SVM Model.
<br>
Storing the performance index into Per_names data frame.
```{r}
Per_names$SVM_RBF <- c(as.numeric(Matrix_svm_rbf$overall[1]), as.numeric(Matrix_svm_rbf$byClass[1]), as.numeric(Matrix_svm_rbf$byClass[2]), as.numeric(Matrix_svm_rbf$byClass[5]), as.numeric(Matrix_svm_rbf$byClass[6]), "Nan", as.numeric(time_SVM_RBF)*60)
```

### Decision Tree
Since we want the decision tree to show the threshold, it would be better to use the original data instead of data with standardization (scale and center). Using the existing train_index to split the dummy_sample dataset (the 18,000 sampled data). Now, we will get a train_DT dataset and a test_DT dataset. The records inside the two datasets are exactly the same as data inside the sample_train and sample_test dataset.
```{r}
train_DT <- dummies_sample[train_index,]
test_DT <- dummies_sample[-train_index,]
```
Now, we can use the train_DT and test_DT to run the model.
```{r eval=FALSE}
time_DT <- Sys.time()
model_DT <- train(Disease.Type ~., 
                  data = train_DT,
                  metric = "Accuracy", 
                  method = "rpart",
                  tuneGrid = expand.grid(cp=seq(0, 0.03, 0.001)),
                  control = rpart.control(minsplit = 30, minbucket = round(30/3), maxdepth = 6),
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3))
time_DT <- Sys.time() - time_DT
save(model_DT, file = "model_DT.RData")
save(time_DT, file = "time_DT.RData")
```

```{r echo=FALSE}
load("model_DT.RData")
load("time_DT.RData")
```
Visualize the model.
```{r}
plot(model_DT)
resampleHist(model_DT)
ggplot(varImp(model_DT, main = "Variable Importance with Decision Tree"))
```
Storing the Hyperparameters.
```{r}
parameter_dt <- c("cp = 0.001, minsplit = 30, minbucket = 10, maxdepth = 6")
```
Let's visualize the conditions.
<br>
In this plot, **High.Blood.Pressure is the most essential indicator** which could be used to predict the disease. Here, we can summarise that people who tends to contract this disease has a higher upper boundary blood pressure (higher than 129). What's more, the elder (whose age is sobe 52) who has a higher level of cholesterol also tends to get this disease. In this plot, **Age and Cholesterol level (specially the high cholesterol level) are the second and thrid important indicators**. The next significant indicators are Glocose level and lower boundary bood pressure (Low.blood.pressure). Even those who are younger than 52, if they has a higer level Cholesterol and a higher Cholesterol level of Glucose level, they are still inside the dangerous group.
```{r fig.height=10, fig.width=13}
fancyRpartPlot(model_DT$finalModel)
```
Here, we can use the test_DT to fit the model and get the Confusion Matrix. The accuracy rate is 0.7264, which seems acceptable. The value of sensitivity is 0.7900, which is good. The specificty (0.6633), however, is relatively lower than previous models. 
```{r}
predict_dt <- predict(model_DT, newdata = test_DT[,-c(13)], type = "prob")
Matrix_dt <- confusionMatrix(as.factor(ifelse(predict_dt$`1`> 0.5, 1, 0)), as.factor(sample_test$Disease.Type))
Matrix_dt
```
**ROC line**
<br>
Plotting Receiver Operating Characteristics (ROC) curve and displaying the value of Area Under the Curve (AUC).
```{r}
roc <- roc(sample_test$Disease.Type, predict_dt$`1`, levels = c(0, 1), direction = "<")
plot(roc)
auc(roc)
```
Storing the performance index into Per_names data frame.
```{r}
Per_names$DT <- c(as.numeric(Matrix_dt$overall[1]), as.numeric(Matrix_dt$byClass[1]), as.numeric(Matrix_dt$byClass[2]), as.numeric(Matrix_dt$byClass[5]), as.numeric(Matrix_dt$byClass[6]), as.numeric(roc$auc), as.numeric(time_DT/60))
```
Utilizing the NBC model to predict the classification result of test data. Storing these results to the result data frame.
```{r warning=FALSE, cache=TRUE}
result$DT <- predict(model_DT, newdata = New_Test, type = "raw")
```
<br>
Then use write.csv function to save the document to local directory.
```{r}
write.csv(result, "xwanyue_hw04_result.csv")
```

```{r}
```
# Model Performance Evaluation
<br>
One should run the models several times with different hyper-parameters in order to optimize the results. To evaluate the performance of these models, we can use **Model Performance Metrics** to calculate the Accuracy, Precision (percent of predicted positive that are correct), Recall (percent of positive cases that are correctly predicted), and ROC. 
<br>
<br>
```{r}

```
### Model Performance Validation
For the Model Performance Validation, there are mainly two types, which are Cross Validation and Bootstrapping. <br>
* **Cross Validation** splits the available dataset to create multiple datasets. <br>
* **Bootstrapping** method uses the original dataset to create multiple datasets after resampling with replacement. <br>
* However, **Bootstrapping it is not as strong as Cross validation** when it is **used for model validation** since that bootstrapping is more about building ensemble models or just estimating parameters. <br> In others words, Cross-validation evaluates how well an algorithm generalizes, whereas bootstrapping actually helps the algorithm to generalize better.<br>
* Hence, **we decide to use 10-fold Crossvalidation for model performance validation** even though Bootstrapping can reduce the variance of a classification algorithm that is based on a single model.
<br>
```{r}

```

### Model Summary
According to the analysis illustrated above, there are the **Summary** of these models:
<br>
As our problem is a part of classification, accuracy will be mesuared instead of the RMSE. "Accuracy" shows how many times a model predicted the right value/ class among all data set. A table which records all the model performance values has been dispalyed below. 
<br>
<br>
* If we only look at the **Accuracy**, ANN2 and XGBoost seem to have a better performance after several times of running (both accuracy value are about 0.7293). Then it should be SVM_RBF, RF, and DT who have a similar accuracy value. ANN1 and LR also have a similar accuracy value. Then it should be KNN and Linear_SVM. NBC generally performed less satisfactory compared with other models. 
<br>
<br>
* In this case, sensitivity is important than specificity due to the nature of this classification. Since we do not want to miss any potential patient, a higher sensitivity value might help us to achive this goal. If we look at **Sensitivity** Value, it is easy to find that SVM_Lieanr perform better than others. The second model is RF while the thrid one is DT. In this case, ANN0 has the lowest value.
<br>
<br>
* Generally, model with a ROC value that is higher than 0.75 is realiable. If we look at **ROC** value, it is good to see that the ROC value of all the models is above 0.75.
<br>
<br>
* If we look at the **running time** value (mins), one can summarise that ANN perforam faster than the rest models. DT and LR are also one of the most agile models in this plot. All of these models mentioned above controls the running time with 1 min. SVM_RBF, however, is the lowest since it took nearly 101 mins to train a dataset which only contains 12,600 records.
<br><br>
Ranking based on Accuracy:
```{r}
Per_names[,order(Per_names[1,] ,decreasing = T)]
```
Ranking based on Sensitivity:
```{r}
Per_names[c(2,6),order(Per_names[2,] ,decreasing = T)]
```
Ranking based on Running time:
```{r}
Per_names[c(7),order(Per_names[7,] ,decreasing = T)]
```
<br>
Next, we can use some function to visualize the modelos performance.
```{r}
model_comparison <- resamples(list(XGBoost = model_XGB, 
                                   LR = model_logistic,
                                   DT = model_DT,
                                   NBC = model_nb,
                                   KNN = model_KNN,
                                   RF = model_RF,
                                   SVM_Linear = model_svm_linear, 
                                   SVM_RBF = model_svm_rbf))
scales <- list(x = list(relation = "free"), y = list(relation = "free"))
bwplot(model_comparison, scales = scales)
```
<br>
<br>
Accoring to the requirement, we can create a data frame called hyper_all to store all the hyper-parameters we used in model tunning.
```{r}
hyper_all <- as.data.frame(c(parameter_ANN0, parameter_ANN1, parameter_ANN2, parameter_LR, parameter_dt, parameter_NBC, parameter_KNN, parameter_RF, parameter_XGB, parameter_svm_linear, parameter_svm_rbf))
colnames(hyper_all) <- c("Hyper-Parameters")
rownames(hyper_all) <- c("ANN0", "ANN1", "ANN2", "Logistic Regression", "Decision Tree", "Naive Bayese", "KNN", "Random Forest", "XGBoost", "SVM_Linear","SVM_RBF")
hyper_all
```

### Comparision between ANN0, Logistic Regression, and Linear SVM
<br>
First of all, we can display the performance result of those three models. As for Accuracy rate, LR model has the highest value while ANN0 has the lowest. If we focus on specificity value,  the value of Liearn_SVM (0.8078) is much higher than that of lr (0.7766), let alone ANN0 (whose sensitivity value is 0.6836). For the ROC value, both LR and ANN0 could be treated as reliable. The value of Linear_SVM is lower than 0.75. For the running time, ANN0 model is the fastest among the three. The next is LR, which also took less than 1 min to train the model. The lowest is Linear_SVM which spend nealy 30 mins.
<br>
<br>
If we want to judge the three models based on Accuracy rate, it will not make much senses since they has a similar accuracy rate. If someone who want a model with a outstanding specificity rate, Linear_SVM is the best bet. If someone is looking for model that cost little of time, LR and ANN0 are suitable.
<br>
<br>
If we want to differantiate the three models based on their theory, one can say that Linear SVMs and logistic regression generally perform comparably in practice.<br>
SVM works well with unstructured and semi-structured data like text and images while logistic regression works with already identified independent variables. SVM is based on geometrical properties of the data while logistic regression is based on statistical approaches. The logistic regression comes from generalized linear regression. The Support Vector Machines algorithm is much more geometrically motivated. Generally, if the number of features is small while the number of train records is large, one can choose logistic regression or linear SVM.<br><br>
For the difference between logistic regression and perceptron, one can say that logistic regression has probabilistic connotations that go beyond the classifier use in ML. The perceptron classification algorithm is a more basic procedure, based on dot products between examples and weights. In some cases, the term perceptron is also used to refer to neural networks which use a logistic function as a transfer function (however, this is not in accordance with the original terminology). In that case, a logistic regression and a "perceptron" are exactly the same. 
```{r}
Per_names[,order(Per_names[1,] ,decreasing = T)][c(7,9,10)]
```
<br>
### Relative Importance of Features Plot
From the plots summary, one can see that, except logistic regression, the top 3 important features are the same in the rest models, which are "high.blood.pressure", "low.blood.pressure, and age". Generally, the fourth important feature is either BMI or Cholesterol Level.
```{r fig.height=20, fig.width=12}
dt <- ggplot(varImp(model_DT, main = "Variable Importance with Decision Tree"))
svm_rbf <- ggplot(varImp(model_svm_rbf, main = "Variable Importance with model_svm_rbf"))
svm_linear <- ggplot(varImp(model_svm_linear), main = "Variable Importance with Linear SVM")
XGB <- ggplot(varImp(model_XGB), main = "Variable Importance with XGB")
RF <- ggplot(varImp(model_RF), main = "Variable Importance with RF")
KNN <- ggplot(varImp(model_KNN), main = "Variable Importance with KNN")
NBC <- ggplot(varImp(model_nb), main = "Variable Importance with NBC")
LR <- ggplot(varImp(model_logistic), main = "Variable Importance with Logistic Regression")

ggarrange(dt, svm_rbf, svm_linear, XGB, RF, KNN, NBC, LR, labels = c("DT","SVM_RBF","SVM_Lieanr","XGBoost", "RF","KNN","NBC","LR"), ncol = 2, nrow = 4)
```

<br>
**Recommendation & Limitation**
<br>
For this case, one should investigate the relationships between hyperperameter and models. Due to the CPU limits, we only use several basic hyper-perameter for model tuning. Hence, we cannot guarantee that we have found out the optimized results. Besides, we only use 20,000 records of data, which also might cause some bias.
<br>

**Finally**
<br>