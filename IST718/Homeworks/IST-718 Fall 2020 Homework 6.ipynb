{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST-718 Fall 2020 Homework 6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLmKS5AHyPcb"
      },
      "source": [
        "##### Grading Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uYE_aKFvp1N"
      },
      "source": [
        "# IST 718: Big Data Analytics\n",
        "\n",
        "- Professor: Willard Williamson <wewillia@syr.edu>\n",
        "- Faculty Assistant: Vidushi Mishra <vmishr01@syr.edu>\n",
        "- Faculty Assistant: Pranav Kottoli Radhakrishna <pkottoli@syr.edu>\n",
        "## General instructions:\n",
        "\n",
        "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
        "- Do not modify cells marked as grading cells or marked as do not modify.\n",
        "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
        "`Runtime `$\\rightarrow$ Factory reset runtime followed by Runtime $\\rightarrow$ Run All.  All runtime errors will result in a minimum penalty of half off.\n",
        "- Google Colab is the official class runtime environment so you should test your code on Colab before submission.\n",
        "- All plots shall include descriptive title and axis labels.  Plot legends shall be included where possible.  Unless stated otherwise, plots can be made using any Python plotting package.  It is understood that spark data structures must be converted to something like numpy or pandas prior to making plots.  All required mathematical operations, filtering, selection, etc., required by a homework question shall be performed in spark prior to converting to numpy or pandas.\n",
        "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells.\n",
        "- Don't add or remove files from your git repo.\n",
        "- Do not change file names in your repo.  This also means don't change the title of the ipython notebook.\n",
        "- You are free to add additional code cells around the cells marked `your code here`.\n",
        "- We reserve the right to take points off for operations that are extremely inefficient or \"heavy weight\".  This is a big data class and extremely inefficient operations make a big difference when scaling up to large data sets.  For example, the spark dataframe collect() method is a very heavy weight operation and should not be used unless it there is a real need for it.  An example where collect() might be needed is to get ready to make a plot after filtering a spark dataframe.\n",
        "- import * is not allowed because it is considered a very bad coding practice and in some cases can result in a significant delay (which slows down the grading process) in loading imports.  For example, the statement `from sympy import *` is not allowed.  You must import the specific packages that you need. \n",
        "- The graders reserve the right to deduct points for subjective things we see with your code.  For example, if we ask you to create a pandas data frame to display values from an investigation and you hard code the values, we will take points off for that.  This is only one of many different things we could find in reviewing your code.  In general, write your code like you are submitting it for a code peer review in industry.  \n",
        "- Level of effort is part of our subjective grading.  For example, in cases where we ask for a more open ended investigation, some students put in significant effort and some students do the minimum possible to meet requirements.  In these cases, we may take points off for students who did not put in much effort as compared to students who put in a lot of effort.  We feel that the students who did a better job deserve a better grade.  We reserve the right to invoke level of effort grading at any time.\n",
        "- Only use spark, spark machine learning, spark data frames, RDD's, and map reduce to solve all problems unless instructed otherwise.\n",
        "- Your notebook must run from start to finish without requiring manual input by the graders.  For example, do not mount your personal Google drive in your notebook as this will require graders to perform manual steps.  In short, your notebook should run from start to finish with no runtime errors and no need for graders to perform any manual steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEjZ4Xye6uLh"
      },
      "source": [
        "## Note that this notebook is expected to run in the Google Colab environment.  All grading for this assignment will take place exclusively in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZzy57K4yPce"
      },
      "source": [
        "This homework proves that diamonds are forever.  In homework 3, we used linear regression to predict diamond prices and evaluated model performance using MSE as the scoring metric.  In this homework, we are going to use the same diamonds data set but this time use decision trees and deep learning to see if we can improve upon the linear regression performance from homework 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP2FPt6_1TWi"
      },
      "source": [
        "# Diamonds Data\n",
        "Just to prove that diamonds are forever, we are going to revisit the diamonds data set.  This homework assignment will use the diamonds dataset to explore random forest decision tree models.\n",
        "\n",
        "The diamonds.csv data set contains 10 columns:\n",
        "- carat: Carat weight of the diamond\n",
        "- cut: Describes cut quality of the diamond. Quality in increasing order Fair, Good, Very Good, Premium, Ideal\n",
        "- color: Color of the diamond, with D being the best and J the worst\n",
        "- clarity: How obvious inclusions are within the diamond:(in order from best to worst, FL = flawless, I3= level 3 inclusions) FL,IF, VVS1, etc.  See this web site for an exhaustive ranking of [clarity](https://4cs.gia.edu/en-us/diamond-clarity/?gclid=Cj0KCQjwnqH7BRDdARIsACTSAduMoc2KQbXkO94BxCfBNC5X8YyjAYcFpWThKQMW46cQj_3p0pZ0o84aAuagEALw_wcB).  The web site has a nice sliding scale you can drag to see the relationship between clarity grades.\n",
        "- depth: depth % - The height of a diamond, measured from the culet to the table, divided by its average girdle diameter\n",
        "- table: table% -  The width of the diamond's table expressed as a percentage of its average diameter\n",
        "- price: The price of the diamond\n",
        "- x: Length (mm)\n",
        "- y: Width (mm)\n",
        "- z: Height (mm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZxH4JRN3Vcj"
      },
      "source": [
        "# Grading Cell\n",
        "enable_grid_search = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDW3ScIk2Q_H"
      },
      "source": [
        "The following cell is used to read the diamonds data set into the colab environment.  Do not change or modify the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A1Gp-HOCSi0",
        "outputId": "a14be220-0bf4-459e-9c64-fd6516b5f555"
      },
      "source": [
        "%%bash\n",
        "# Do not change or modify this file\n",
        "# Need to install pyspark\n",
        "# if pyspark is already installed, will print a message indicating pyspark already isntalled\n",
        "pip install pyspark\n",
        "\n",
        "# Download the data files from github\n",
        "# If the data file does not exist in the colab environment\n",
        "if [[ ! -f ./quotes_by_char.csv ]]; then \n",
        "   # download the data file from github and save it in this colab environment instance\n",
        "   wget https://raw.githubusercontent.com/wewilli1/ist718_data/master/diamonds.csv  \n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "Collecting py4j==0.10.9\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py): started\n",
            "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=376990ba05faf4b183fe17fe5c9518c28d76b0cbbb93037b21d94a6be106603f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-11-24 03:37:53--  https://raw.githubusercontent.com/wewilli1/ist718_data/master/diamonds.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3192560 (3.0M) [text/plain]\n",
            "Saving to: ‘diamonds.csv’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  1% 3.84M 1s\n",
            "    50K .......... .......... .......... .......... ..........  3% 9.97M 1s\n",
            "   100K .......... .......... .......... .......... ..........  4% 4.02M 1s\n",
            "   150K .......... .......... .......... .......... ..........  6% 4.86M 1s\n",
            "   200K .......... .......... .......... .......... ..........  8% 16.1M 0s\n",
            "   250K .......... .......... .......... .......... ..........  9% 61.9M 0s\n",
            "   300K .......... .......... .......... .......... .......... 11% 16.0M 0s\n",
            "   350K .......... .......... .......... .......... .......... 12% 11.8M 0s\n",
            "   400K .......... .......... .......... .......... .......... 14% 15.7M 0s\n",
            "   450K .......... .......... .......... .......... .......... 16% 18.9M 0s\n",
            "   500K .......... .......... .......... .......... .......... 17% 18.8M 0s\n",
            "   550K .......... .......... .......... .......... .......... 19% 16.9M 0s\n",
            "   600K .......... .......... .......... .......... .......... 20% 16.0M 0s\n",
            "   650K .......... .......... .......... .......... .......... 22% 20.1M 0s\n",
            "   700K .......... .......... .......... .......... .......... 24% 24.4M 0s\n",
            "   750K .......... .......... .......... .......... .......... 25% 10.9M 0s\n",
            "   800K .......... .......... .......... .......... .......... 27% 18.6M 0s\n",
            "   850K .......... .......... .......... .......... .......... 28% 19.2M 0s\n",
            "   900K .......... .......... .......... .......... .......... 30% 18.1M 0s\n",
            "   950K .......... .......... .......... .......... .......... 32% 20.4M 0s\n",
            "  1000K .......... .......... .......... .......... .......... 33% 18.1M 0s\n",
            "  1050K .......... .......... .......... .......... .......... 35% 15.2M 0s\n",
            "  1100K .......... .......... .......... .......... .......... 36% 28.5M 0s\n",
            "  1150K .......... .......... .......... .......... .......... 38% 18.5M 0s\n",
            "  1200K .......... .......... .......... .......... .......... 40% 32.6M 0s\n",
            "  1250K .......... .......... .......... .......... .......... 41% 14.4M 0s\n",
            "  1300K .......... .......... .......... .......... .......... 43% 16.3M 0s\n",
            "  1350K .......... .......... .......... .......... .......... 44% 30.8M 0s\n",
            "  1400K .......... .......... .......... .......... .......... 46% 15.0M 0s\n",
            "  1450K .......... .......... .......... .......... .......... 48% 20.4M 0s\n",
            "  1500K .......... .......... .......... .......... .......... 49% 33.9M 0s\n",
            "  1550K .......... .......... .......... .......... .......... 51% 18.7M 0s\n",
            "  1600K .......... .......... .......... .......... .......... 52% 15.5M 0s\n",
            "  1650K .......... .......... .......... .......... .......... 54% 21.9M 0s\n",
            "  1700K .......... .......... .......... .......... .......... 56% 30.4M 0s\n",
            "  1750K .......... .......... .......... .......... .......... 57% 20.6M 0s\n",
            "  1800K .......... .......... .......... .......... .......... 59% 26.0M 0s\n",
            "  1850K .......... .......... .......... .......... .......... 60% 15.8M 0s\n",
            "  1900K .......... .......... .......... .......... .......... 62% 5.16M 0s\n",
            "  1950K .......... .......... .......... .......... .......... 64% 27.8M 0s\n",
            "  2000K .......... .......... .......... .......... .......... 65% 42.3M 0s\n",
            "  2050K .......... .......... .......... .......... .......... 67% 17.7M 0s\n",
            "  2100K .......... .......... .......... .......... .......... 68% 32.5M 0s\n",
            "  2150K .......... .......... .......... .......... .......... 70% 17.1M 0s\n",
            "  2200K .......... .......... .......... .......... .......... 72% 19.0M 0s\n",
            "  2250K .......... .......... .......... .......... .......... 73% 28.8M 0s\n",
            "  2300K .......... .......... .......... .......... .......... 75% 17.1M 0s\n",
            "  2350K .......... .......... .......... .......... .......... 76% 27.9M 0s\n",
            "  2400K .......... .......... .......... .......... .......... 78% 24.6M 0s\n",
            "  2450K .......... .......... .......... .......... .......... 80% 23.4M 0s\n",
            "  2500K .......... .......... .......... .......... .......... 81% 22.7M 0s\n",
            "  2550K .......... .......... .......... .......... .......... 83% 28.1M 0s\n",
            "  2600K .......... .......... .......... .......... .......... 84% 24.1M 0s\n",
            "  2650K .......... .......... .......... .......... .......... 86% 32.2M 0s\n",
            "  2700K .......... .......... .......... .......... .......... 88% 32.1M 0s\n",
            "  2750K .......... .......... .......... .......... .......... 89% 16.8M 0s\n",
            "  2800K .......... .......... .......... .......... .......... 91% 20.0M 0s\n",
            "  2850K .......... .......... .......... .......... .......... 93% 37.8M 0s\n",
            "  2900K .......... .......... .......... .......... .......... 94% 43.6M 0s\n",
            "  2950K .......... .......... .......... .......... .......... 96% 21.6M 0s\n",
            "  3000K .......... .......... .......... .......... .......... 97% 19.1M 0s\n",
            "  3050K .......... .......... .......... .......... .......... 99% 45.1M 0s\n",
            "  3100K .......... .......                                    100%  211M=0.2s\n",
            "\n",
            "2020-11-24 03:37:53 (16.6 MB/s) - ‘diamonds.csv’ saved [3192560/3192560]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgVybwMy2zfd"
      },
      "source": [
        "# Question 0 (0 pts)\n",
        "Please provide the following the data so we can easily correlate your notebook with the grade book:\n",
        "- Your Name: Wanyue Xiao\n",
        "- Your github user name: xwanyue0221\n",
        "- Your SU email address: xwanyue@syr.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFz91GzJ4XFj"
      },
      "source": [
        "Your grade for grid search problems in this assignment will be determined in part on level of effort and your model performance results as compared to other students in the class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2XNxzBB2aZU"
      },
      "source": [
        "# Question 1 (10 pts)\n",
        "Read the diamonds.csv file into a spark data frame named `diamonds_df`.  Perform feature engineering as needed for training decision trees.  Name the new data frame diamonds_df_xformed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYtA8Bkw3Aq1"
      },
      "source": [
        "# your code here\n",
        "%matplotlib inline\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "spark = SparkSession \\\n",
        "  .builder \\\n",
        "  .master(\"local[*]\")\\\n",
        "  .config(\"spark.memory.fraction\", 0.8) \\\n",
        "  .config(\"spark.executor.memory\", \"12g\") \\\n",
        "  .config(\"spark.driver.memory\", \"12g\")\\\n",
        "  .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
        "  .config(\"spark.memory.offHeap.size\",\"12g\")\\\n",
        "  .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)\n",
        "import os\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StringIndexerModel\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# read the csv file\n",
        "diamonds_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"./diamonds.csv\")\n",
        "# drop the unnamed column\n",
        "diamonds_df = diamonds_df.drop('_c0')\n",
        "categoricalColumns = ['cut', 'color', 'clarity']\n",
        "numericCols = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
        "\n",
        "# get the distinct values of each categorical column\n",
        "cut_list = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\n",
        "color_list = ['J','I','H','G','F','E','D']\n",
        "clarity_list = ['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF']\n",
        "# To convert the three categorical data into ordinal numeric data, we need to define the StringIndexerModel pipeline respectively\n",
        "pipe_1 = Pipeline(stages=[StringIndexerModel.from_labels(cut_list, inputCol=\"cut\", outputCol=\"cut_idx\")])\n",
        "pipe_2 = Pipeline(stages=[StringIndexerModel.from_labels(color_list, inputCol=\"color\", outputCol=\"color_idx\")])\n",
        "pipe_3 = Pipeline(stages=[StringIndexerModel.from_labels(clarity_list, inputCol=\"clarity\", outputCol=\"clarity_idx\")])\n",
        "# combine and encapsulate all the transformation codes into one pipeline\n",
        "feature_engineering_pipe = Pipeline(stages=[pipe_1, pipe_2, pipe_3])\n",
        "result = feature_engineering_pipe.fit(diamonds_df).transform(diamonds_df)\n",
        "# drop the original categorical columns and change the column names\n",
        "result = result.drop(*categoricalColumns)\n",
        "diamonds_df_xformed = result.toDF(*(c.replace('_idx', '') for c in result.columns))\n",
        "\n",
        "# vector assembler\n",
        "prepro_pipe = Pipeline(stages=[VectorAssembler(inputCols=diamonds_df_xformed.drop('price').columns, outputCol='features')])\n",
        "diamonds_df_xformed = prepro_pipe.fit(diamonds_df_xformed).transform(diamonds_df_xformed)\n",
        "\n",
        "# get train and test subset\n",
        "train, test = diamonds_df_xformed.randomSplit([0.7, 0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDKlAHzK3GkZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a636c00c-2b12-4fbd-f771-105c2eace241"
      },
      "source": [
        "# Grading Cell - do not modify\n",
        "display(diamonds_df_xformed.toPandas().head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>carat</th>\n",
              "      <th>depth</th>\n",
              "      <th>table</th>\n",
              "      <th>price</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "      <th>cut</th>\n",
              "      <th>color</th>\n",
              "      <th>clarity</th>\n",
              "      <th>features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.23</td>\n",
              "      <td>61.5</td>\n",
              "      <td>55.0</td>\n",
              "      <td>326</td>\n",
              "      <td>3.95</td>\n",
              "      <td>3.98</td>\n",
              "      <td>2.43</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[0.23, 61.5, 55.0, 3.95, 3.98, 2.43, 4.0, 5.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>59.8</td>\n",
              "      <td>61.0</td>\n",
              "      <td>326</td>\n",
              "      <td>3.89</td>\n",
              "      <td>3.84</td>\n",
              "      <td>2.31</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.21, 59.8, 61.0, 3.89, 3.84, 2.31, 3.0, 5.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.23</td>\n",
              "      <td>56.9</td>\n",
              "      <td>65.0</td>\n",
              "      <td>327</td>\n",
              "      <td>4.05</td>\n",
              "      <td>4.07</td>\n",
              "      <td>2.31</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[0.23, 56.9, 65.0, 4.05, 4.07, 2.31, 1.0, 5.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.29</td>\n",
              "      <td>62.4</td>\n",
              "      <td>58.0</td>\n",
              "      <td>334</td>\n",
              "      <td>4.20</td>\n",
              "      <td>4.23</td>\n",
              "      <td>2.63</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[0.29, 62.4, 58.0, 4.2, 4.23, 2.63, 3.0, 1.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.31</td>\n",
              "      <td>63.3</td>\n",
              "      <td>58.0</td>\n",
              "      <td>335</td>\n",
              "      <td>4.34</td>\n",
              "      <td>4.35</td>\n",
              "      <td>2.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[0.31, 63.3, 58.0, 4.34, 4.35, 2.75, 1.0, 0.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   carat  depth  ...  clarity                                           features\n",
              "0   0.23   61.5  ...      1.0  [0.23, 61.5, 55.0, 3.95, 3.98, 2.43, 4.0, 5.0,...\n",
              "1   0.21   59.8  ...      2.0  [0.21, 59.8, 61.0, 3.89, 3.84, 2.31, 3.0, 5.0,...\n",
              "2   0.23   56.9  ...      4.0  [0.23, 56.9, 65.0, 4.05, 4.07, 2.31, 1.0, 5.0,...\n",
              "3   0.29   62.4  ...      3.0  [0.29, 62.4, 58.0, 4.2, 4.23, 2.63, 3.0, 1.0, ...\n",
              "4   0.31   63.3  ...      1.0  [0.31, 63.3, 58.0, 4.34, 4.35, 2.75, 1.0, 0.0,...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-N1EcFY3CG2"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er1V2eZ23Hrp"
      },
      "source": [
        "The following questions will create a random forest regressor model, train the model using a grid search, and use the model for inference.  The goal is to see if we can improve upon the linear regression score from homework 3. You can find the spark documentation for the random forest regressor [here](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XwLgVI34uP6"
      },
      "source": [
        "# Question 2 (20 pts)\n",
        "Create and train your random forest regressor model using a grid search in the cell below.  You are free to use K-Fold Cross validation if you wish.  Your grid search must be entirely encapsulated in the `if enable_grid_search` if statement.  The `enable_grid_search` Boolean is defined in a grading cell above.  You will disable the grid search before you submit by setting enable_grid_search to false.  Setting enable_grid_search to false should not result in a runtime error.  You will not receive full credit if any part of your grid search is outside of the if statement or if runtime errros result from setting the `enable_grid_search` variable to false."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gfVl99j47Sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f51fa9-f128-4119-9921-d668e8935ea6"
      },
      "source": [
        "# your code here\n",
        "\n",
        "# initialize a random forest transformer\n",
        "rf = RandomForestRegressor(featuresCol = 'features', labelCol = 'price', minInstancesPerNode = 30)\n",
        "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "\n",
        "if enable_grid_search:\n",
        "    # building parameter grid \n",
        "    paramGrid = (ParamGridBuilder()\n",
        "                 .addGrid(rf.maxDepth, [15, 20, 30])\n",
        "                 .addGrid(rf.maxBins, [30, 50])\n",
        "                 .addGrid(rf.numTrees, [20, 30, 50]).build())\n",
        "    \n",
        "    cv = CrossValidator(estimator=rf, \n",
        "                        estimatorParamMaps=paramGrid, \n",
        "                        evaluator=evaluator, \n",
        "                        numFolds=3)\n",
        "    \n",
        "    cvModel = cv.fit(train)\n",
        "    predictions = cvModel.bestModel.transform(test)\n",
        "    \n",
        "    print(\"mse\", evaluator.evaluate(predictions))\n",
        "    print('numTrees - ', cvModel.bestModel.getNumTrees)\n",
        "    print('maxDepth - ', cvModel.bestModel.getOrDefault('maxDepth'))\n",
        "    print('maxBins - ', cvModel.bestModel.getOrDefault('maxBins'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mse 451312.24664500024\n",
            "numTrees -  30\n",
            "maxDepth -  30\n",
            "maxBins -  50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVFHESWM544r"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQcJEwqw57e6"
      },
      "source": [
        "# Question 3 (20 pts)\n",
        "Create a pipeline named `best_pipe` that hard codes the tuning parameters from the best model found by the grid search in question 2 above.  Train and test best_pipe.  Do not use k-fold cross validation in question 3.  Clearly print the resulting train and test MSE for best_pipe so it's easy for the graders to see your resulting MSEs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPuAzFg2B7xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50173f77-3042-47c9-df23-d28f39917ff2"
      },
      "source": [
        "# Your code here\n",
        "rf_pipe = Pipeline(stages=[RandomForestRegressor(featuresCol = 'features', labelCol = 'price', \n",
        "                                                 maxBins = 50, numTrees = 30, maxDepth = 20, minInstancesPerNode = 30)])\n",
        "rfBestModel = rf_pipe.fit(train)\n",
        "rfBestPredictions = rfBestModel.transform(test)\n",
        "\n",
        "print(\"MSE for training dataset: \" + str(evaluator.evaluate(rfBestModel.transform(train))))\n",
        "print(\"MSE for testing dataset: \" + str(evaluator.evaluate(rfBestPredictions)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE for training dataset: 435798.26959282585\n",
            "MSE for testing dataset: 462100.0243813725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN6MAsAZ77vr"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feuCoidR8HqE"
      },
      "source": [
        "# Question 4 (20 pts)\n",
        "Use your best_pipe pipeline in question 3 for inference.  Create a pandas data frame named `rf_feature_importance` which contains 2 columns: `feature`, and `importance`.  Load the feature column with the feature name and the importance column with the feature importance score as determined by the random forest model. Sort the feature importances from high to low such that the most important feature is in the first row of the data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6pdzDU58JN0"
      },
      "source": [
        "# your code here\n",
        "import pandas as pd\n",
        "\n",
        "rf_model = rfBestModel.stages[-1]\n",
        "importances = rf_model.featureImportances.toArray()\n",
        "feature_list = diamonds_df_xformed.drop('price').columns\n",
        "\n",
        "# create a pandas dataframe\n",
        "rf_feature_importance = pd.DataFrame(list(zip(feature_list, importances)), columns =['features', 'importance']).sort_values('importance', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXD8OV-J9MSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "f3b261fb-e124-4a0e-cd9e-22fda1f6b266"
      },
      "source": [
        "# grading cell - do not modify\n",
        "display(rf_feature_importance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>carat</td>\n",
              "      <td>0.317879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>y</td>\n",
              "      <td>0.265153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>x</td>\n",
              "      <td>0.214129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>z</td>\n",
              "      <td>0.117791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>clarity</td>\n",
              "      <td>0.051510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>color</td>\n",
              "      <td>0.026332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>cut</td>\n",
              "      <td>0.002638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>depth</td>\n",
              "      <td>0.002383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>table</td>\n",
              "      <td>0.002185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  features  importance\n",
              "0    carat    0.317879\n",
              "4        y    0.265153\n",
              "3        x    0.214129\n",
              "5        z    0.117791\n",
              "8  clarity    0.051510\n",
              "7    color    0.026332\n",
              "6      cut    0.002638\n",
              "1    depth    0.002383\n",
              "2    table    0.002185"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM7mpmBD86Dk"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CffkMqE-Y-JM"
      },
      "source": [
        "# Question 5 (20 pts)\n",
        "Write code to print the decision logic for any of the trees in the forest from the best_pipe pipeline.  Copy the printed decision text to the tree printout markdown cell below and retain the same formatting and indentation as the code printout so it's easy for the graders to view the data.  You need to double click the \"Your Decision Tree Print Out Here\" markdown cell and paste your output inside the two sets of triple quotes. The triple quotes are jupyter markdown indicating you want to present code.  Essentially, replace the text inside the triple quotes with your tree printout.  Solutions that do not maintain readable formatting will not receive full credit.\n",
        "\n",
        "Add comments to the markdown cell below describing how the root node is split:  Describe 2 things in the markdown cell.  1) What specific predictor variable is being split and what is the value that determines the left / right split.  2) We need you to paste the tree decision logic output from your run in the markdown cell because the top level split may change from run to run.  If the graders run your notebook, the top level split for the tree may be different than the top level split from when you made the run.  Describe why the top level predictor changes from run to run.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdSHouQAY-JO",
        "scrolled": false
      },
      "source": [
        "# your code here\n",
        "rf_default = Pipeline(stages=[RandomForestRegressor(featuresCol = 'features', labelCol = 'price')]).fit(train).stages[-1]\n",
        "# print(rf_default.trees[0].toDebugString)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRhj27DYY-JR"
      },
      "source": [
        " ```\n",
        "Your Decision Tree Print Out Here - \n",
        "\n",
        "DecisionTreeRegressionModel: uid=dtr_0fcbdc6c42b5, depth=5, numNodes=63, numFeatures=9\n",
        "  If (feature 0 <= 0.975)\n",
        "   If (feature 0 <= 0.605)\n",
        "    If (feature 4 <= 5.005)\n",
        "     If (feature 0 <= 0.355)\n",
        "      If (feature 3 <= 4.305)\n",
        "       Predict: 591.3299015219337\n",
        "      Else (feature 3 > 4.305)\n",
        "       Predict: 742.0420976229359\n",
        "     Else (feature 0 > 0.355)\n",
        "      If (feature 3 <= 4.755)\n",
        "       Predict: 870.8565436241611\n",
        "      Else (feature 3 > 4.755)\n",
        "       Predict: 1031.3978240302743\n",
        "    Else (feature 4 > 5.005)\n",
        "     If (feature 8 in {0.0,1.0,2.0,3.0,4.0})\n",
        "      If (feature 0 <= 0.525)\n",
        "       Predict: 1464.4464864864865\n",
        "      Else (feature 0 > 0.525)\n",
        "       Predict: 1615.2197855750487\n",
        "     Else (feature 8 not in {0.0,1.0,2.0,3.0,4.0})\n",
        "      If (feature 7 in {0.0,1.0,2.0,3.0})\n",
        "       Predict: 1925.928813559322\n",
        "      Else (feature 7 not in {0.0,1.0,2.0,3.0})\n",
        "       Predict: 2483.018390804598\n",
        "   Else (feature 0 > 0.605)\n",
        "    If (feature 4 <= 5.955)\n",
        "     If (feature 7 in {0.0,1.0,2.0})\n",
        "      If (feature 4 <= 5.665)\n",
        "       Predict: 1955.8513931888544\n",
        "      Else (feature 4 > 5.665)\n",
        "       Predict: 2401.265097236438\n",
        "     Else (feature 7 not in {0.0,1.0,2.0})\n",
        "      If (feature 0 <= 0.715)\n",
        "       Predict: 2637.2137698603756\n",
        "      Else (feature 0 > 0.715)\n",
        "       Predict: 2963.474683544304\n",
        "    Else (feature 4 > 5.955)\n",
        "     If (feature 4 <= 6.135)\n",
        "      If (feature 3 <= 5.955)\n",
        "       Predict: 3170.4883720930234\n",
        "      Else (feature 3 > 5.955)\n",
        "       Predict: 3482.8038379530917\n",
        "     Else (feature 4 > 6.135)\n",
        "      If (feature 6 in {0.0,1.0,3.0})\n",
        "       Predict: 3802.2535211267605\n",
        "      Else (feature 6 not in {0.0,1.0,3.0})\n",
        "       Predict: 4182.627352572145\n",
        "  Else (feature 0 > 0.975)\n",
        "   If (feature 0 <= 1.505)\n",
        "    If (feature 8 in {0.0,1.0,2.0})\n",
        "     If (feature 8 in {0.0,1.0})\n",
        "      If (feature 4 <= 6.985)\n",
        "       Predict: 4384.502228163993\n",
        "      Else (feature 4 > 6.985)\n",
        "       Predict: 6683.794871794872\n",
        "     Else (feature 8 not in {0.0,1.0})\n",
        "      If (feature 5 <= 4.305)\n",
        "       Predict: 5211.367283950617\n",
        "      Else (feature 5 > 4.305)\n",
        "       Predict: 8525.851449275362\n",
        "    Else (feature 8 not in {0.0,1.0,2.0})\n",
        "     If (feature 8 in {3.0,4.0})\n",
        "      If (feature 4 <= 6.985)\n",
        "       Predict: 6641.082848837209\n",
        "      Else (feature 4 > 6.985)\n",
        "       Predict: 10076.048327137547\n",
        "     Else (feature 8 not in {3.0,4.0})\n",
        "      If (feature 5 <= 4.0649999999999995)\n",
        "       Predict: 9087.389432485323\n",
        "      Else (feature 5 > 4.0649999999999995)\n",
        "       Predict: 10829.599670510708\n",
        "   Else (feature 0 > 1.505)\n",
        "    If (feature 3 <= 7.835)\n",
        "     If (feature 6 in {0.0,1.0,2.0,3.0})\n",
        "      If (feature 8 in {0.0,1.0})\n",
        "       Predict: 8534.397163120568\n",
        "      Else (feature 8 not in {0.0,1.0})\n",
        "       Predict: 11535.609347442682\n",
        "     Else (feature 6 not in {0.0,1.0,2.0,3.0})\n",
        "      If (feature 5 <= 4.8149999999999995)\n",
        "       Predict: 11967.138436482084\n",
        "      Else (feature 5 > 4.8149999999999995)\n",
        "       Predict: 14996.153846153846\n",
        "    Else (feature 3 > 7.835)\n",
        "     If (feature 1 <= 63.849999999999994)\n",
        "      If (feature 7 in {0.0})\n",
        "       Predict: 14051.72340425532\n",
        "      Else (feature 7 not in {0.0})\n",
        "       Predict: 15275.223468507334\n",
        "     Else (feature 1 > 63.849999999999994)\n",
        "      If (feature 3 <= 8.295)\n",
        "       Predict: 11624.786666666667\n",
        "      Else (feature 3 > 8.295)\n",
        "       Predict: 14696.733333333334\n",
        "\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zk7F0xfY-JR"
      },
      "source": [
        "Your explanation here:\n",
        "<br>\n",
        "The first predictor (feature 0) is 'carat' and the variable that determines the split is '0.975'. The algorithm is an ensemble model, creating a ‘forest’ of many decision ‘trees’ with the number of trees defined by the user. Each decision tree is created based on a subset of columns and observations rows from the dataset. These trees are grown using the training data set and applied to the test dataset. The final classification returned by the model is the one which matches the classifications provided by the greatest number of individual decision trees. Given that Random Forest Algorithmn will randomly sample features during each split when bulding the tree, each decision tree within the algorithm is created using a different, ‘random’ subset of attributes and observations from the original training dataset. Therefore, the result will change each time it has been run. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY9dGVEr9ild"
      },
      "source": [
        "# Question 6 (5 pts)\n",
        "Describe if the random forest model MSE score was better or worse than the MSE score from you best model in homework 3.  Include both scores in your description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow6DkSXa97hd"
      },
      "source": [
        "Your improvement explanation here:  \n",
        "<br>\n",
        "The MSE score of Random Forest (which is 462100.0243813725) is way much better than that of Linear Regression Model accomplished in homework 3 (which is 1452635.051274)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIKoaPczcLbm"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a7l7Rae7637"
      },
      "source": [
        "# Question 7 (5 pts)\n",
        "Set the `enable_grid_search` Boolean variable to False in the grading cell at the top of this notebook.  Perform a __Runtime -> factory reset__, __Runtime -> Run all__ test to verify there are no runtime errors.  Leave the `enable_grid_search` variable set to False and turn in your assignment.  This is the kind of thing you should be doing before you turn in every assignment. Remember this for future classes and when you get a job in industry.  This question will be graded as all or nothing.  You ether set the Boolean correct or not.  Additional points will be deducted elsewhere for runtime errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H7AIcpJ96Y7"
      },
      "source": [
        "# Extra Credit (10 pts)\n",
        "This homework was intended to take less time to complete and be about half the effort of previous assignments.  This doesn't allow us to explore GBT or deep learning.  \n",
        "\n",
        "For extra credit, train a GBT or Deep Learning model using a grid search.  Protect the grid search inside the if enable_grid_search statement in the first code cell below.  You are free to use K-Fold cross validation if you wish.  The spark documentation for GBM can be found [here](https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier).  The spark documentation for deep learning can be found [here](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier)\n",
        "\n",
        "In the second code cell below, hard code the best model parameters as determined by the grid search in a new pipeline named `best_pipe_2`.  Train and test `best_pipe_2` and save your resulting test MSE in a variable.  Do not use K-Fold cross validation when training best_pipe_2.  \n",
        "\n",
        "In the third code cell below, create a pandas data frame named `compare_1_df` which contains 2 columns: Model and MSE.  Populate the Model column with model names: LR, RF, GBT or DL.  Populate the score column with the linear regression, random forest, and gradient boosted tree or deep learning test MSE scores. The linear regression score is from homework 3. The random forest score is from the random forest model above.  The GBT or Deep Learning score is from this extra credit problem.  Sort compare_1_df such that the best score is in the first row of the data frame. \n",
        "\n",
        "To get full credit, your GBT or deep learning solution should produce a score as good or better than the random forest score above.  In addition, the same rules as above apply where all of your grid search code shall be protected by the enable_grid_search Boolean.  Code that produces a runtime error when enable_grid_search is set to False will get 0 credit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjoUMwaRWi3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281974dd-0a3b-4c6a-8e00-e3735b80886a"
      },
      "source": [
        "# Your GBT / Deep Learning grid search code here\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "gbt = GBTRegressor(labelCol = 'price', featuresCol = 'features')\n",
        "\n",
        "if enable_grid_search:\n",
        "    # generate the grid object, which iterates over different combinations of paramters\n",
        "    gbt_paramGrid = (ParamGridBuilder()\\\n",
        "                     .addGrid(gbt.maxDepth, [2, 4, 9, 12])\n",
        "                     .build())\n",
        "    \n",
        "    # generate a 3-fold cross validation model\n",
        "    gbt_cv = CrossValidator(estimator = gbt,\n",
        "                            estimatorParamMaps = gbt_paramGrid,\n",
        "                            evaluator = evaluator,\n",
        "                            numFolds = 3)\n",
        "    \n",
        "    gbtModel = gbt_cv.fit(train)\n",
        "    predictions = gbtModel.bestModel.transform(test)\n",
        "    \n",
        "    print(\"mse\", evaluator.evaluate(predictions))\n",
        "    print('maxDepth - ', gbtModel.bestModel.getOrDefault('maxDepth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mse 403312.8032579468\n",
            "maxDepth -  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCgt5D0kedOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27514a93-e8e1-41c4-b742-47a796714fba"
      },
      "source": [
        "# your hard coded parameter best model code here\n",
        "gbt_pipeline = Pipeline(stages=[GBTRegressor(featuresCol = 'features', labelCol = 'price', maxDepth = 9)])\n",
        "gbtBestModel = gbt_pipeline.fit(train)\n",
        "gbtBestPredictions = gbtBestModel.transform(test)\n",
        "\n",
        "print(\"MSE for training dataset: \" + str(evaluator.evaluate(gbtBestModel.transform(train))))\n",
        "print(\"MSE for testing dataset: \" + str(evaluator.evaluate(gbtBestPredictions)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE for training dataset: 201754.8090042882\n",
            "MSE for testing dataset: 403312.8032579468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXeWOkdNiMFb"
      },
      "source": [
        "# Create compare_1_df\n",
        "model_names = ['LR', 'RF', 'XGBoost']\n",
        "mse_values = ['1452635.051274', '462100.0243813725', '403312.8032579468']\n",
        "\n",
        "compare_1_df = pd.DataFrame(list(zip(model_names, mse_values)), columns =['Model', 'MSE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqwK_dTdgxlC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "9c7b28e2-a77d-4ecf-825d-c5318f86d27b"
      },
      "source": [
        "# Grading cell do not modify\n",
        "display(compare_1_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>MSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LR</td>\n",
              "      <td>1452635.051274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RF</td>\n",
              "      <td>462100.0243813725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>403312.8032579468</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Model                MSE\n",
              "0       LR     1452635.051274\n",
              "1       RF  462100.0243813725\n",
              "2  XGBoost  403312.8032579468"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16UBIv4DfDLL"
      },
      "source": [
        ""
      ]
    }
  ]
}